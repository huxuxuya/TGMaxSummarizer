"""
Ollama AI –ø—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
"""
import asyncio
import aiohttp
import json
from typing import List, Dict, Optional, Any
from .base import BaseAIProvider

class OllamaProvider(BaseAIProvider):
    """–ü—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ —á–µ—Ä–µ–∑ Ollama API"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.base_url = config.get('base_url', 'http://localhost:11434')
        self.model = config.get('model', 'deepseek-r1:8b')
        self.timeout = config.get('timeout', 600)  # –¢–∞–π–º–∞—É—Ç –¥–ª—è Ollama –∑–∞–ø—Ä–æ—Å–æ–≤ (10 –º–∏–Ω—É—Ç)
        
        # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        self.logger.info(f"üîó DEBUG: OllamaProvider –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å base_url = {self.base_url}")
        self.logger.info(f"üîó DEBUG: OllamaProvider –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å model = {self.model}")
    
    def set_model(self, model_name: str):
        """
        –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        
        Args:
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'gemma3:12b')
        """
        print(f"üîç DEBUG: OllamaProvider.set_model –≤—ã–∑–≤–∞–Ω —Å model_name: {model_name}")
        self.logger.info(f"üîç DEBUG: OllamaProvider.set_model –≤—ã–∑–≤–∞–Ω —Å model_name: {model_name}")
        
        self.model = model_name
        print(f"üîç DEBUG: OllamaProvider.model —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤: {self.model}")
        self.logger.info(f"üîç DEBUG: OllamaProvider.model —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤: {self.model}")
        self.logger.info(f"üîó –ú–æ–¥–µ–ª—å Ollama –∏–∑–º–µ–Ω–µ–Ω–∞ –Ω–∞: {model_name}")
        
    async def summarize_chat(self, messages: List[Dict], chat_context: Optional[Dict] = None) -> str:
        """
        –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —á–∞—Ç–∞ —Å –ø–æ–º–æ—â—å—é Ollama
        
        Args:
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
            chat_context: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —á–∞—Ç–∞
            
        Returns:
            –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –≤ –≤–∏–¥–µ —Å—Ç—Ä–æ–∫–∏
        """
        try:
            # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
            self.logger.info("üîß –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è Ollama...")
            optimized_messages = self.optimize_text(messages)
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            formatted_text = self.format_messages_for_analysis(optimized_messages)
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è (–ü–û–°–õ–ï –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏)
            if self.llm_logger:
                self.llm_logger.log_formatted_messages(formatted_text, len(optimized_messages))
            
            self.logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è Ollama:")
            self.logger.info(f"   –í—Å–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π: {len(messages)}")
            self.logger.info(f"   –ü–æ—Å–ª–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {len(optimized_messages)}")
            self.logger.info(f"   –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {len(formatted_text)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –í—ã–∑—ã–≤–∞–µ–º Ollama API
            self.logger.info(f"ü§ñ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –≤ Ollama ({self.model})...")
            
            # –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –µ—Å–ª–∏ –ª–æ–≥–≥–µ—Ä —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
            if self.llm_logger:
                self.llm_logger.log_llm_request(formatted_text, "summarization")
            
            import time
            start_time = time.time()
            summary = await self._call_ollama_api(formatted_text)
            end_time = time.time()
            response_time = end_time - start_time
            
            # –õ–æ–≥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –µ—Å–ª–∏ –ª–æ–≥–≥–µ—Ä —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
            if self.llm_logger and summary:
                self.llm_logger.log_llm_response(summary, "summarization", response_time)
                self.llm_logger.log_stage_time('summarization', response_time)
            
            if summary:
                self.logger.info("‚úÖ –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∞ –æ—Ç Ollama")
                return summary
            else:
                self.logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—é–º–µ –æ—Ç Ollama")
                return "‚ùå –û—à–∏–±–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ Ollama"
                
        except Exception as e:
            error_msg = f"‚ùå –û—à–∏–±–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ Ollama: {e}"
            self.logger.error(error_msg)
            # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É –µ—Å–ª–∏ –µ—Å—Ç—å –ª–æ–≥–≥–µ—Ä
            if self.llm_logger:
                self.llm_logger.log_error("summarization", error_msg, str(e))
            return f"‚ùå –û—à–∏–±–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏: {str(e)}"
    
    async def is_available(self) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama
        
        Returns:
            True –µ—Å–ª–∏ Ollama —Å–µ—Ä–≤–µ—Ä –¥–æ—Å—Ç—É–ø–µ–Ω, False –∏–Ω–∞—á–µ
        """
        if not self.validate_config():
            return False
        
        try:
            self.logger.info(f"üîó DEBUG: –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama –ø–æ URL: {self.base_url}/api/tags")
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=10)) as session:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama —Å–µ—Ä–≤–µ—Ä–∞
                async with session.get(f"{self.base_url}/api/tags") as response:
                    if response.status == 200:
                        data = await response.json()
                        models = [model['name'] for model in data.get('models', [])]
                        
                        # –î–ª—è Ollama –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —á—Ç–æ —Å–µ—Ä–≤–µ—Ä –¥–æ—Å—Ç—É–ø–µ–Ω –∏ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å
                        if models:
                            self.logger.info(f"‚úÖ Ollama –¥–æ—Å—Ç—É–ø–µ–Ω, –Ω–∞–π–¥–µ–Ω–æ {len(models)} –º–æ–¥–µ–ª–µ–π")
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –Ω—É–∂–Ω–∞—è –º–æ–¥–µ–ª—å (–¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏)
                            if self.model in models:
                                self.logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é {self.model} –Ω–∞–π–¥–µ–Ω–∞")
                            else:
                                self.logger.info(f"‚ÑπÔ∏è –ú–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é {self.model} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –Ω–æ –µ—Å—Ç—å –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏")
                            return True
                        else:
                            self.logger.warning(f"‚ö†Ô∏è Ollama –¥–æ—Å—Ç—É–ø–µ–Ω, –Ω–æ –Ω–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")
                            return False
                    else:
                        self.logger.error(f"‚ùå Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, —Å—Ç–∞—Ç—É—Å: {response.status}")
                        return False
                        
        except aiohttp.ClientError as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama: {e}")
            return False
        except Exception as e:
            self.logger.error(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ Ollama: {e}")
            return False
    
    async def generate_response(self, prompt: str, stream_callback=None) -> Optional[str]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        
        Args:
            prompt: –¢–µ–∫—Å—Ç –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞
            stream_callback: –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Ç–æ–∫–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö (text_chunk)
            
        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            import time
            start_time = time.time()
            
            self.logger.info(f"ü§ñ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ Ollama –Ω–∞ –ø—Ä–æ–º–ø—Ç –¥–ª–∏–Ω–æ–π {len(prompt)} —Å–∏–º–≤–æ–ª–æ–≤")
            self.logger.debug(f"=== GENERATE_RESPONSE INPUT ===")
            self.logger.debug(f"Prompt length: {len(prompt)}")
            self.logger.debug(f"Prompt preview: {prompt[:200]}...")
            self.logger.debug(f"=== END INPUT ===")
            
            # –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –µ—Å–ª–∏ –ª–æ–≥–≥–µ—Ä —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
            if self.llm_logger:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É –ø—Ä–æ–º–ø—Ç–∞
                request_type = "reflection" if "—Ä–µ—Ñ–ª–µ–∫—Å–∏—è" in prompt.lower() or "–∞–Ω–∞–ª–∏–∑" in prompt.lower() else "improvement"
                self.llm_logger.log_llm_request(prompt, request_type)
            
            response = await self._call_ollama_api(prompt, is_generation=True, stream_callback=stream_callback)
            
            end_time = time.time()
            response_time = end_time - start_time
            
            if response:
                self.logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç Ollama –¥–ª–∏–Ω–æ–π {len(response)} —Å–∏–º–≤–æ–ª–æ–≤ –∑–∞ {response_time:.2f}—Å")
                self.logger.debug(f"=== GENERATE_RESPONSE OUTPUT ===")
                self.logger.debug(f"Response length: {len(response)}")
                self.logger.debug(f"Response preview: {response[:200]}...")
                self.logger.debug(f"=== END OUTPUT ===")
                
                # –õ–æ–≥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –µ—Å–ª–∏ –ª–æ–≥–≥–µ—Ä —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
                if self.llm_logger:
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –æ—Ç–≤–µ—Ç–∞ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É –ø—Ä–æ–º–ø—Ç–∞
                    request_type = "reflection" if "—Ä–µ—Ñ–ª–µ–∫—Å–∏—è" in prompt.lower() or "–∞–Ω–∞–ª–∏–∑" in prompt.lower() else "improvement"
                    self.llm_logger.log_llm_response(response, request_type, response_time)
                    
                    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —ç—Ç–∞–ø–∞
                    if request_type == "reflection":
                        self.llm_logger.log_stage_time('reflection', response_time)
                    elif request_type == "improvement":
                        self.llm_logger.log_stage_time('improvement', response_time)
                
                return response
            else:
                self.logger.warning("‚ö†Ô∏è Ollama –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç")
                return None
                
        except Exception as e:
            error_msg = f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ Ollama: {e}"
            self.logger.error(error_msg)
            # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É –µ—Å–ª–∏ –µ—Å—Ç—å –ª–æ–≥–≥–µ—Ä
            if self.llm_logger:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É –ø—Ä–æ–º–ø—Ç–∞
                request_type = "reflection" if "—Ä–µ—Ñ–ª–µ–∫—Å–∏—è" in prompt.lower() or "–∞–Ω–∞–ª–∏–∑" in prompt.lower() else "improvement"
                self.llm_logger.log_error(request_type, error_msg, str(e))
            return None
    
    def get_provider_info(self) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–µ Ollama
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–µ
        """
        return {
            'name': 'Ollama',
            'display_name': 'Ollama (–õ–æ–∫–∞–ª—å–Ω–∞—è)',
            'description': f'–õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å {self.model} —á–µ—Ä–µ–∑ Ollama',
            'version': self.model,
            'max_tokens': 8000,
            'supports_streaming': True,
            'api_endpoint': self.base_url,
            'provider_type': 'ollama',
            'is_local': True
        }
    
    def validate_config(self) -> bool:
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ Ollama
        
        Returns:
            True –µ—Å–ª–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≤–∞–ª–∏–¥–Ω–∞, False –∏–Ω–∞—á–µ
        """
        if not self.base_url:
            self.logger.error("‚ùå Ollama base URL –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
            return False
        
        if not self.model:
            self.logger.error("‚ùå Ollama –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞")
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ URL –≤—ã–≥–ª—è–¥–∏—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
        if not (self.base_url.startswith('http://') or self.base_url.startswith('https://')):
            self.logger.error("‚ùå Ollama base URL –¥–æ–ª–∂–µ–Ω –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å http:// –∏–ª–∏ https://")
            return False
        
        return True
    
    async def _call_ollama_api(self, text: str, is_generation: bool = False, stream_callback=None) -> Optional[str]:
        """
        –í—ã–∑–≤–∞—Ç—å Ollama API –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –∏–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            is_generation: True –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, False –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
            stream_callback: –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Ç–æ–∫–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö (text_chunk)
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            if is_generation:
                # –î–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –ø—Ä–æ–º–ø—Ç
                prompt = text
            else:
                # –î–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç
                from prompts import PromptTemplates
                prompt = PromptTemplates.get_summarization_prompt(text, 'ollama')

            self.logger.info(f"üîó –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –≤ Ollama")
            self.logger.info(f"üîó DEBUG: URL –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: {self.base_url}/api/generate")
            self.logger.info(f"üìù –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
            self.logger.info(f"ü§ñ –ú–æ–¥–µ–ª—å: {self.model}")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º streaming –µ—Å–ª–∏ –µ—Å—Ç—å callback
            use_streaming = stream_callback is not None
            
            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": use_streaming,
                "options": {
                    "temperature": 0.7
                }
            }
            
            self.logger.info(f"üîó DEBUG: –û—Ç–ø—Ä–∞–≤–ª—è–µ–º POST –∑–∞–ø—Ä–æ—Å —Å —Ç–∞–π–º–∞—É—Ç–æ–º {self.timeout}—Å")
            self.logger.info(f"üîó DEBUG: Payload: {payload}")
            
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=self.timeout)) as session:
                self.logger.info(f"üîó DEBUG: –°–æ–∑–¥–∞–Ω–∞ —Å–µ—Å—Å–∏—è, –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å...")
                async with session.post(
                    f"{self.base_url}/api/generate",
                    json=payload,
                    headers={"Content-Type": "application/json"}
                ) as response:
                    self.logger.info(f"üîó DEBUG: –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º {response.status}")
                    
                    if response.status == 200:
                        if use_streaming:
                            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
                            full_response = ""
                            async for line in response.content:
                                line_str = line.decode('utf-8').strip()
                                if line_str:
                                    try:
                                        chunk_data = json.loads(line_str)
                                        if 'response' in chunk_data:
                                            chunk_text = chunk_data['response']
                                            full_response += chunk_text
                                            
                                            # –í—ã–∑—ã–≤–∞–µ–º callback –¥–ª—è –∞–Ω–∏–º–∞—Ü–∏–∏
                                            if stream_callback:
                                                stream_callback(chunk_text)
                                            
                                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ
                                            if chunk_data.get('done', False):
                                                break
                                    except json.JSONDecodeError:
                                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ JSON —Å—Ç—Ä–æ–∫–∏
                                        continue
                            
                            result = full_response.strip()
                            self.logger.info(f"üì° –ü–æ–ª—É—á–µ–Ω –ø–æ—Ç–æ–∫–æ–≤—ã–π –æ—Ç–≤–µ—Ç –æ—Ç Ollama: {len(result)} —Å–∏–º–≤–æ–ª–æ–≤")
                            return result
                        else:
                            # –û–±—ã—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–Ω–µ streaming)
                            data = await response.json()
                            self.logger.info(f"üîó DEBUG: –ü–æ–ª—É—á–µ–Ω JSON –æ—Ç–≤–µ—Ç –æ—Ç Ollama: {data}")
                            
                            if 'response' in data:
                                result = data['response'].strip()
                                self.logger.info(f"üì° –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç Ollama: {len(result)} —Å–∏–º–≤–æ–ª–æ–≤")
                                return result
                            else:
                                self.logger.error("‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç Ollama")
                                self.logger.error(f"üîó DEBUG: –ö–ª—é—á–∏ –≤ –æ—Ç–≤–µ—Ç–µ: {list(data.keys())}")
                                return None
                    else:
                        error_text = await response.text()
                        self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ Ollama API: {response.status} - {error_text}")
                        self.logger.error(f"üîó DEBUG: –ü–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç —Å–µ—Ä–≤–µ—Ä–∞: {error_text}")
                        return None
                        
        except asyncio.TimeoutError:
            error_msg = f"‚ùå –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –∫ Ollama (>{self.timeout}—Å)"
            self.logger.error(error_msg)
            # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É –µ—Å–ª–∏ –µ—Å—Ç—å –ª–æ–≥–≥–µ—Ä
            if self.llm_logger:
                self.llm_logger.log_error("extraction", error_msg, None)
            return None
        except aiohttp.ClientError as e:
            error_msg = f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama: {e}"
            self.logger.error(error_msg)
            self.logger.error(f"üîó DEBUG: –¢–∏–ø –æ—à–∏–±–∫–∏: {type(e)}")
            # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É
            if self.llm_logger:
                self.llm_logger.log_error("extraction", error_msg, str(e))
            return None
        except json.JSONDecodeError as e:
            error_msg = f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –æ—Ç Ollama: {e}"
            self.logger.error(error_msg)
            self.logger.error(f"üîó DEBUG: –¢–∏–ø –æ—à–∏–±–∫–∏: {type(e)}")
            # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É
            if self.llm_logger:
                self.llm_logger.log_error("extraction", error_msg, str(e))
            return None
        except Exception as e:
            error_msg = f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ Ollama: {e}"
            self.logger.error(error_msg)
            self.logger.error(f"üîó DEBUG: –¢–∏–ø –æ—à–∏–±–∫–∏: {type(e)}")
            self.logger.error(f"üîó DEBUG: –ê—Ä–≥—É–º–µ–Ω—Ç—ã –æ—à–∏–±–∫–∏: {e.args}")
            # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É
            if self.llm_logger:
                self.llm_logger.log_error("extraction", error_msg, str(e))
            return None
    
    async def get_available_models(self) -> Dict[str, Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π Ollama
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –º–æ–¥–µ–ª—è—Ö
        """
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(f"{self.base_url}/api/tags", timeout=aiohttp.ClientTimeout(total=10)) as response:
                    if response.status == 200:
                        data = await response.json()
                        models = {}
                        
                        if 'models' in data:
                            for model_info in data['models']:
                                model_name = model_info.get('name', '')
                                if model_name:
                                    # –°–æ–∑–¥–∞–µ–º –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º–æ–µ –∏–º—è
                                    display_name = model_name.replace(':', ' ').title()
                                    
                                    models[model_name] = {
                                        'display_name': display_name,
                                        'name': model_name,
                                        'size': model_info.get('size', 0),
                                        'modified_at': model_info.get('modified_at', ''),
                                        'free': True  # –õ–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤—Å–µ–≥–¥–∞ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ
                                    }
                        
                        self.logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(models)} –º–æ–¥–µ–ª–µ–π Ollama")
                        return models
                    else:
                        error_text = await response.text()
                        self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π Ollama: {response.status} - {error_text}")
                        return {}
                        
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π Ollama: {e}")
            return {}
