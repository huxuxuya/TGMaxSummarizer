#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
"""
import os

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –î–û –≤—Å–µ—Ö –∏–º–ø–æ—Ä—Ç–æ–≤!
os.environ['OLLAMA_BASE_URL'] = 'http://192.168.1.75:11434'
print(f"üîó DEBUG: –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º OLLAMA_BASE_URL = {os.environ['OLLAMA_BASE_URL']}")

import asyncio
import logging
import time
import aiohttp
import json
import sys
import select
import tty
import termios
from database import DatabaseManager
from chat_analyzer import ChatAnalyzer

class ProgressChatAnalyzer(ChatAnalyzer):
    """
    –û–±–µ—Ä—Ç–∫–∞ –Ω–∞–¥ ChatAnalyzer –¥–ª—è –ø–æ–∫–∞–∑–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    """
    def __init__(self, config, show_generation=False, animation_speed=0.02):
        super().__init__(config)
        self.show_generation = show_generation
        self.animation_speed = animation_speed
    
    async def analyze_chat_with_specific_model(self, messages, provider_name, model_id, user_id=None, 
                                             enable_reflection=False, clean_data_first=False):
        """
        –ê–Ω–∞–ª–∏–∑ —á–∞—Ç–∞ —Å –ø–æ–∫–∞–∑–æ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        """
        if not self.show_generation:
            return await super().analyze_chat_with_specific_model(
                messages, provider_name, model_id, user_id, enable_reflection, clean_data_first
            )
        
        print_progress_stage("üìä –ê–Ω–∞–ª–∏–∑ —á–∞—Ç–∞", f"–ú–æ–¥–µ–ª—å: {model_id}")
        
        # –°–æ–∑–¥–∞–µ–º LLM –ª–æ–≥–≥–µ—Ä –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —Ä–µ–∂–∏–º–∞
        from llm_logger import LLMLogger
        import os
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ü–µ–Ω–∞—Ä–∏–π
        if clean_data_first:
            scenario = "with_cleaning"
        elif enable_reflection:
            scenario = "with_reflection"
        else:
            scenario = "without_reflection"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≤ –æ–∫—Ä—É–∂–µ–Ω–∏–∏ —Ñ–ª–∞–≥ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —Ä–µ–∂–∏–º–∞
        test_mode = os.environ.get('TEST_MODE') == 'true'
        
        # –°–æ–∑–¥–∞–µ–º –ª–æ–≥–≥–µ—Ä
        llm_logger = LLMLogger(
            scenario=scenario,
            model_name=model_id,
            test_mode=test_mode
        )
        llm_logger.set_session_info(provider_name, model_id, None, user_id)
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä
        if provider_name == 'ollama' and 'ollama' in self.config:
            provider = self.provider_factory.create_provider(provider_name, self.config['ollama'])
        else:
            provider = self.provider_factory.create_provider(provider_name, self.config)
        if not provider:
            print("‚ùå –ü—Ä–æ–≤–∞–π–¥–µ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return None
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å
        if hasattr(provider, 'set_model'):
            provider.set_model(model_id)
        
        # –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        if clean_data_first:
            print_progress_stage("üßπ –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö", "–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π...")
            
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏
            temp_chat_context = {
                'total_messages': len(messages),
                'date': messages[0].get('message_time', 0) if messages else 0,
                'provider': provider_name,
                'model': model_id
            }
            
            messages = await self.clean_messages(provider, messages, temp_chat_context, llm_logger)
            if not messages:
                print("‚ùå –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ —É–¥–∞–ª–∞—Å—å –∏–ª–∏ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å —Å–æ–æ–±—â–µ–Ω–∏–π")
                return None
            print(f"‚úÖ –û—á–∏—â–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–π: {len(messages)}")
        
        # –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è
        print_progress_stage("üìù –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è", "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ä–µ–∑—é–º–µ...")
        summary = await provider.summarize_chat(messages)
        
        if self.show_generation and summary:
            print_with_animation(summary, "üìù –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è", self.animation_speed)
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if llm_logger and summary:
            llm_logger.log_raw_result(summary)
            llm_logger.log_formatted_result(summary)
            llm_logger.log_session_summary({
                'summary': summary,
                'messages_count': len(messages),
                'model': model_id,
                'provider': provider_name
            })
        
        if not enable_reflection:
            return summary
        
        # –†–µ—Ñ–ª–µ–∫—Å–∏—è
        print_progress_stage("ü§î –†–µ—Ñ–ª–µ–∫—Å–∏—è", "–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏...")
        reflection = await provider.generate_response(
            f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–∞—á–µ—Å—Ç–≤–æ —ç—Ç–æ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –∏ –ø—Ä–µ–¥–ª–æ–∂–∏ —É–ª—É—á—à–µ–Ω–∏—è:\n\n{summary}"
        )
        
        if self.show_generation and reflection:
            print_with_animation(reflection, "ü§î –†–µ—Ñ–ª–µ–∫—Å–∏—è", self.animation_speed)
        
        # –£–ª—É—á—à–µ–Ω–∏–µ
        print_progress_stage("‚ú® –£–ª—É—á—à–µ–Ω–∏–µ", "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏...")
        improved = await provider.generate_response(
            f"–£–ª—É—á—à–∏ —ç—Ç—É —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞:\n\n–°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è:\n{summary}\n\n–ê–Ω–∞–ª–∏–∑:\n{reflection}"
        )
        
        if self.show_generation and improved:
            print_with_animation(improved, "‚ú® –£–ª—É—á—à–µ–Ω–∏–µ", self.animation_speed)
        
        return {
            'summary': summary,
            'reflection': reflection,
            'improved': improved
        }
    
    async def structured_analysis_with_specific_model(self, messages, provider_name, model_name, user_id):
        """
        –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –ø–æ–∫–∞–∑–æ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        """
        if not self.show_generation:
            return await super().structured_analysis_with_specific_model(
                messages, provider_name, model_name, user_id
            )
        
        print_progress_stage("üèóÔ∏è –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑", f"–ú–æ–¥–µ–ª—å: {model_name}")
        
        # –°–æ–∑–¥–∞–µ–º LLM Logger –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        import os
        from llm_logger import LLMLogger
        
        test_mode = os.environ.get('TEST_MODE') == 'true'
        llm_logger = LLMLogger(
            scenario="structured_analysis",
            model_name=model_name,
            test_mode=test_mode
        )
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–µ—Å—Å–∏–∏
        llm_logger.set_session_info(provider_name, model_name, None, user_id)
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä
        if provider_name == 'ollama' and 'ollama' in self.config:
            provider = self.provider_factory.create_provider(provider_name, self.config['ollama'])
        else:
            provider = self.provider_factory.create_provider(provider_name, self.config)
        if not provider:
            print("‚ùå –ü—Ä–æ–≤–∞–π–¥–µ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return None
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å
        if hasattr(provider, 'set_model'):
            provider.set_model(model_name)
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        print_progress_stage("üè∑Ô∏è –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è", "–ê–Ω–∞–ª–∏–∑ —Ç–∏–ø–æ–≤ —Å–æ–æ–±—â–µ–Ω–∏–π...")
        
        # –°–æ–∑–¥–∞–µ–º callback –¥–ª—è –ø–æ—Ç–æ–∫–æ–≤–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        def classification_callback(chunk):
            if self.show_generation:
                print(chunk, end='', flush=True)
        
        classification = await self._classify_messages(provider, messages, llm_logger, stream_callback=classification_callback)
        
        if self.show_generation and classification:
            print()  # –ù–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –ø–æ—Å–ª–µ –∞–Ω–∏–º–∞—Ü–∏–∏
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π - –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥
        relevant_messages = self._filter_by_classification(messages, classification)
        
        print(f"‚úÖ –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π: {len(relevant_messages)} –∏–∑ {len(messages)}")
        
        # –≠–∫—Å—Ç—Ä–∞–∫—Ü–∏—è —Å–ª–æ—Ç–æ–≤
        print_progress_stage("üîç –≠–∫—Å—Ç—Ä–∞–∫—Ü–∏—è —Å–ª–æ—Ç–æ–≤", "–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–æ–±—ã—Ç–∏–π –∏ —Ñ–∞–∫—Ç–æ–≤...")
        
        def extraction_callback(chunk):
            if self.show_generation:
                print(chunk, end='', flush=True)
        
        events = await self._extract_slots(provider, relevant_messages, classification, llm_logger, stream_callback=extraction_callback)
        
        if self.show_generation and events:
            print()  # –ù–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –ø–æ—Å–ª–µ –∞–Ω–∏–º–∞—Ü–∏–∏
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–≤–æ–¥–∫–∏ –¥–ª—è —Ä–æ–¥–∏—Ç–µ–ª–µ–π
        print_progress_stage("üë®‚Äçüë©‚Äçüëß‚Äçüë¶ –°–≤–æ–¥–∫–∞ –¥–ª—è —Ä–æ–¥–∏—Ç–µ–ª–µ–π", "–°–æ–∑–¥–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞...")
        
        def summary_callback(chunk):
            if self.show_generation:
                print(chunk, end='', flush=True)
        
        summary = await self._generate_parent_summary(provider, events, llm_logger, stream_callback=summary_callback)
        
        if self.show_generation and summary:
            print()  # –ù–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –ø–æ—Å–ª–µ –∞–Ω–∏–º–∞—Ü–∏–∏
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if llm_logger and summary:
            llm_logger.log_raw_result(summary)
            llm_logger.log_formatted_result(summary)
            llm_logger.log_session_summary({
                'summary': summary,
                'events': events,
                'classification': classification
            })
        
        return {
            'summary': summary,
            'events': events,
            'classification': classification
        }

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def estimate_tokens(text: str) -> int:
    """
    –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ
    (–ø—Ä–∏–º–µ—Ä–Ω–æ 4 —Å–∏–º–≤–æ–ª–∞ = 1 —Ç–æ–∫–µ–Ω –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞)
    """
    return len(text) // 4

def calculate_tokens_per_second(response_text: str, duration: float) -> float:
    """
    –í—ã—á–∏—Å–ª–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É
    """
    if duration <= 0:
        return 0.0
    
    tokens = estimate_tokens(response_text)
    return tokens / duration

def format_duration(seconds: float) -> str:
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –≤—Ä–µ–º—è –≤ —á–∏—Ç–∞–µ–º—ã–π –≤–∏–¥
    """
    if seconds < 60:
        return f"{seconds:.1f}—Å"
    elif seconds < 3600:
        minutes = int(seconds // 60)
        secs = seconds % 60
        return f"{minutes}–º {secs:.1f}—Å"
    else:
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = seconds % 60
        return f"{hours}—á {minutes}–º {secs:.1f}—Å"

def print_with_animation(text: str, prefix: str = "ü§ñ", delay: float = 0.02):
    """
    –í—ã–≤–æ–¥–∏—Ç—å —Ç–µ–∫—Å—Ç —Å –∞–Ω–∏–º–∞—Ü–∏–µ–π –ø–µ—á–∞—Ç–∏
    """
    print(f"\n{prefix} –ì–µ–Ω–µ—Ä–∞—Ü–∏—è:")
    print("-" * 50)
    
    # –ú–≥–Ω–æ–≤–µ–Ω–Ω—ã–π —Ä–µ–∂–∏–º
    if delay == 0:
        print(text)
        print("-" * 50)
        return
    
    print("üí° –ù–∞–∂–º–∏—Ç–µ –ª—é–±—É—é –∫–ª–∞–≤–∏—à—É –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∞–Ω–∏–º–∞—Ü–∏–∏")
    print("-" * 50)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å–ª–∏—à–∫–æ–º –ª–∏ –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∏–º–∞—Ü–∏–∏
    if len(text) > 1000 and delay > 0.01:
        print("‚ö†Ô∏è –î–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç, —É—Å–∫–æ—Ä—è–µ–º –∞–Ω–∏–º–∞—Ü–∏—é...")
        delay = min(delay, 0.01)
    
    for i, char in enumerate(text):
        print(char, end='', flush=True)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–∂–∞—Ç–∏–µ –∫–ª–∞–≤–∏—à–∏ –∫–∞–∂–¥—ã–µ 10 —Å–∏–º–≤–æ–ª–æ–≤
        if i % 10 == 0 and check_key_pressed():
            key = get_key()
            if key:
                print(f"\n‚ö° –ê–Ω–∏–º–∞—Ü–∏—è —É—Å–∫–æ—Ä–µ–Ω–∞! (–Ω–∞–∂–∞—Ç–∞ –∫–ª–∞–≤–∏—à–∞: {repr(key)})")
                delay = 0.001  # –£—Å–∫–æ—Ä—è–µ–º –∞–Ω–∏–º–∞—Ü–∏—é
        
        time.sleep(delay)
    
    print("\n" + "-" * 50)

def print_progress_stage(stage: str, description: str = ""):
    """
    –í—ã–≤–æ–¥–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–∫—É—â–µ–º —ç—Ç–∞–ø–µ
    """
    print(f"\nüîÑ {stage}")
    if description:
        print(f"   {description}")
    print("-" * 30)

def check_key_pressed():
    """
    –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –Ω–∞–∂–∞—Ç–∞ –ª–∏ –∫–ª–∞–≤–∏—à–∞ (–Ω–µ–±–ª–æ–∫–∏—Ä—É—é—â–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞)
    """
    try:
        if sys.platform == 'win32':
            import msvcrt
            return msvcrt.kbhit()
        else:
            return select.select([sys.stdin], [], [], 0) == ([sys.stdin], [], [])
    except:
        return False

def get_key():
    """
    –ü–æ–ª—É—á–∏—Ç—å –Ω–∞–∂–∞—Ç—É—é –∫–ª–∞–≤–∏—à—É
    """
    try:
        if sys.platform == 'win32':
            import msvcrt
            return msvcrt.getch().decode('utf-8')
        else:
            return sys.stdin.read(1)
    except:
        return None

async def get_available_models(base_url: str) -> list:
    """
    –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —Å–µ—Ä–≤–µ—Ä–∞ Ollama
    """
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(f"{base_url}/api/tags", timeout=10) as response:
                if response.status == 200:
                    data = await response.json()
                    models = []
                    for model_info in data.get('models', []):
                        model_name = model_info.get('name', '')
                        if model_name:
                            models.append(model_name)
                    return models
                else:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: HTTP {response.status}")
                    return []
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama: {e}")
        return []

async def main():
    print("ü§ñ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏")
    print("‚ú® –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:")
    print("   ‚Ä¢ –ü–æ–∫–∞–∑ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ (–≤–∫–ª—é—á–µ–Ω –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)")
    print("   ‚Ä¢ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –±—ã—Å—Ç—Ä—ã–π –≤—ã–≤–æ–¥ –±–µ–∑ –∑–∞–¥–µ—Ä–∂–µ–∫")
    print("   ‚Ä¢ –ü–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç Ollama API")
    print("   ‚Ä¢ –ü–æ–¥—Ä–æ–±–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å –ø–æ —ç—Ç–∞–ø–∞–º\n")
    
    # 1. –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —Å–µ—Ä–≤–µ—Ä–∞ Ollama
    base_url = os.environ.get('OLLAMA_BASE_URL', 'http://localhost:11434')
    print(f"üîç –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π —Å —Å–µ—Ä–≤–µ—Ä–∞ {base_url}...")
    
    available_models = await get_available_models(base_url)
    
    if not available_models:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –∏–ª–∏ —Å–µ—Ä–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
        print("üí° –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω –∏ –¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ –∞–¥—Ä–µ—Å—É:", base_url)
        return
    
    # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–µ–π
    models = {}
    for i, model in enumerate(available_models, 1):
        models[str(i)] = model
    
    print(f"\n‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(available_models)} –º–æ–¥–µ–ª–µ–π:")
    for key, model in models.items():
        print(f"  {key}. {model}")
    
    choice = input(f"\n–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å (1-{len(available_models)}): ").strip()
    model_name = models.get(choice)
    
    if not model_name:
        print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä")
        return
    
    print(f"\n‚úÖ –í—ã–±—Ä–∞–Ω–∞ –º–æ–¥–µ–ª—å: {model_name}\n")
    
    # 2. –í—ã–±–æ—Ä —Å—Ü–µ–Ω–∞—Ä–∏—è
    print("–î–æ—Å—Ç—É–ø–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏:")
    print("  1. –ë–µ–∑ —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏ (–±—ã—Å—Ç—Ä–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è)")
    print("  2. –° —Ä–µ—Ñ–ª–µ–∫—Å–∏–µ–π (–∞–Ω–∞–ª–∏–∑ –∏ —É–ª—É—á—à–µ–Ω–∏–µ)")
    print("  3. –° –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ—á–∏—Å—Ç–∫–æ–π (—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è + —Ä–µ—Ñ–ª–µ–∫—Å–∏—è)")
    print("  4. –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è + —ç–∫—Å—Ç—Ä–∞–∫—Ü–∏—è + —Å–≤–æ–¥–∫–∞)")
    print("  5. –í—Å–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ (—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ)")
    
    scenario_choice = input("\n–í—ã–±–µ—Ä–∏—Ç–µ —Å—Ü–µ–Ω–∞—Ä–∏–π (1-5): ").strip()
    
    if scenario_choice not in ['1', '2', '3', '4', '5']:
        print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä —Å—Ü–µ–Ω–∞—Ä–∏—è")
        return
    
    print(f"\n‚úÖ –í—ã–±—Ä–∞–Ω —Å—Ü–µ–Ω–∞—Ä–∏–π: {scenario_choice}\n")
    
    # 2.5. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è (–≤–∫–ª—é—á–µ–Ω–æ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
    show_generation = True  # –í—Å–µ–≥–¥–∞ –≤–∫–ª—é—á–µ–Ω–æ
    animation_speed = 0.02  # –ù–æ—Ä–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å
    
    print("‚úÖ –ü–æ–∫–∞–∑ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤–∫–ª—é—á–µ–Ω –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
    print(f"‚úÖ –°–∫–æ—Ä–æ—Å—Ç—å –∞–Ω–∏–º–∞—Ü–∏–∏: {animation_speed}s –Ω–∞ —Å–∏–º–≤–æ–ª (–Ω–æ—Ä–º–∞–ª—å–Ω–æ)")
    
    print()
    
    # 3. –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ë–î
    try:
        db = DatabaseManager('bot_database.db')
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –≥—Ä—É–ø–ø—ã –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        all_groups = db.get_all_groups()
        
        if not all_groups:
            print("‚ùå –ù–µ—Ç –≥—Ä—É–ø–ø –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö")
            return
        
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –≥—Ä—É–ø–ø—É
        chat_info = all_groups[0]
        group_id = chat_info['group_id']
        chat_name = chat_info.get('group_name', 'Unknown')
        
        print(f"üì± –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≥—Ä—É–ø–ø–∞: {chat_name} (ID: {group_id})")
        
        # –ü–æ–ª—É—á–∞–µ–º vk_chat_id –¥–ª—è —ç—Ç–æ–π –≥—Ä—É–ø–ø—ã
        vk_chats = db.get_group_vk_chats(group_id)
        if not vk_chats:
            print("‚ùå –ù–µ—Ç —Å–≤—è–∑–∞–Ω–Ω—ã—Ö VK —á–∞—Ç–æ–≤ –¥–ª—è —ç—Ç–æ–π –≥—Ä—É–ø–ø—ã")
            return
        
        vk_chat_id = vk_chats[0]['chat_id']
        print(f"üì± –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è VK —á–∞—Ç: {vk_chat_id}")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –∑–∞ 13.10.2025
        messages = db.get_messages_by_date(vk_chat_id, "2025-10-16")
        
        if not messages:
            print("‚ùå –ù–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–π –∑–∞ 13.10.2025")
            return
        
        print(f"üìä –ù–∞–π–¥–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–π: {len(messages)}\n")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö: {e}")
        return
    
    # 4. –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —Ä–µ–∂–∏–º–∞
    os.environ['TEST_MODE'] = 'true'
    os.environ['TEST_MODEL_NAME'] = model_name
    os.environ['ENABLE_LLM_LOGGING'] = 'true'
    
    print(f"üîó DEBUG: –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è OLLAMA_BASE_URL = {os.environ.get('OLLAMA_BASE_URL')}")
    
    # 5. –°–æ–∑–¥–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å —è–≤–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
    try:
        # –ü–µ—Ä–µ–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —è–≤–Ω–æ, —á—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
        from config import AI_PROVIDERS
        analyzer = ProgressChatAnalyzer(AI_PROVIDERS, show_generation=show_generation, animation_speed=animation_speed)
        print(f"üîó DEBUG: ProgressChatAnalyzer —Å–æ–∑–¥–∞–Ω —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π: {analyzer.config.get('ollama', {}).get('base_url', '–ù–ï –ù–ê–ô–î–ï–ù–û')}")
        print(f"üîó DEBUG: –ü–æ–∫–∞–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {'–≤–∫–ª—é—á–µ–Ω' if show_generation else '–æ—Ç–∫–ª—é—á–µ–Ω'}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞: {e}")
        return
    
    # 6. –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ü–µ–Ω–∞—Ä–∏–∏ –¥–ª—è –∑–∞–ø—É—Å–∫–∞
    all_scenarios = [
        ("1_without_reflection", False, False, False, "–ë–µ–∑ —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏"),
        ("2_with_reflection", True, False, False, "–° —Ä–µ—Ñ–ª–µ–∫—Å–∏–µ–π"),
        ("3_with_cleaning", True, True, False, "–° –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ—á–∏—Å—Ç–∫–æ–π"),
        ("4_structured_analysis", False, False, True, "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
    ]
    
    # –í—ã–±–∏—Ä–∞–µ–º —Å—Ü–µ–Ω–∞—Ä–∏–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤—ã–±–æ—Ä–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    if scenario_choice == '1':
        scenarios_to_run = [all_scenarios[0]]
    elif scenario_choice == '2':
        scenarios_to_run = [all_scenarios[1]]
    elif scenario_choice == '3':
        scenarios_to_run = [all_scenarios[2]]
    elif scenario_choice == '4':
        scenarios_to_run = [all_scenarios[3]]
    else:  # scenario_choice == '5'
        scenarios_to_run = all_scenarios
    
    results = {}
    
    # 7. –ó–∞–ø—É—Å–∫–∞–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏
    performance_stats = {}
    
    for scenario_name, enable_reflection, clean_data_first, structured_analysis, description in scenarios_to_run:
        print(f"üîÑ –ó–∞–ø—É—Å–∫ —Å—Ü–µ–Ω–∞—Ä–∏—è: {description} ({scenario_name})...")
        
        start_time = time.time()
        
        try:
            if structured_analysis:
                # –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑
                result = await analyzer.structured_analysis_with_specific_model(
                    messages=messages,
                    provider_name="ollama",
                    model_name=model_name,
                    user_id=None
                )
            else:
                # –û–±—ã—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑
                result = await analyzer.analyze_chat_with_specific_model(
                    messages=messages,
                    provider_name="ollama",
                    model_id=model_name,
                    user_id=None,
                    enable_reflection=enable_reflection,
                    clean_data_first=clean_data_first
                )
            
            end_time = time.time()
            duration = end_time - start_time
            
            if result:
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤
                if isinstance(result, dict):
                    if structured_analysis:
                        # –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑
                        summary_text = result.get('summary', '') or ''
                        events = result.get('events', [])
                        classification = result.get('classification', [])
                        
                        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ç–æ–∫–µ–Ω—ã –¥–ª—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
                        events_text = str(events) if events else ''
                        classification_text = str(classification) if classification else ''
                        
                        total_tokens = estimate_tokens(summary_text) + estimate_tokens(events_text) + estimate_tokens(classification_text)
                        tokens_per_sec = calculate_tokens_per_second(summary_text + events_text + classification_text, duration)
                    else:
                        # –û–±—ã—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å —Ä–µ—Ñ–ª–µ–∫—Å–∏–µ–π
                        summary_text = result.get('summary', '') or ''
                        reflection_text = result.get('reflection', '') or ''
                        improved_text = result.get('improved', '') or ''
                        
                        total_tokens = estimate_tokens(summary_text) + estimate_tokens(reflection_text) + estimate_tokens(improved_text)
                        tokens_per_sec = calculate_tokens_per_second(summary_text + reflection_text + improved_text, duration)
                else:
                    # –ï—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç - —Å—Ç—Ä–æ–∫–∞ (–±–µ–∑ —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏)
                    result_text = str(result) if result else ''
                    total_tokens = estimate_tokens(result_text)
                    tokens_per_sec = calculate_tokens_per_second(result_text, duration)
                
                performance_stats[scenario_name] = {
                    'duration': duration,
                    'tokens': total_tokens,
                    'tokens_per_sec': tokens_per_sec,
                    'success': True
                }
                
                print(f"‚úÖ –°—Ü–µ–Ω–∞—Ä–∏–π {description} –∑–∞–≤–µ—Ä—à–µ–Ω")
                print(f"‚è±Ô∏è  –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {format_duration(duration)}")
                print(f"üî¢ –¢–æ–∫–µ–Ω–æ–≤ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: ~{total_tokens}")
                print(f"üöÄ –°–∫–æ—Ä–æ—Å—Ç—å: {tokens_per_sec:.1f} —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫")
                results[scenario_name] = "–£—Å–ø–µ—à–Ω–æ"
            else:
                end_time = time.time()
                duration = end_time - start_time
                performance_stats[scenario_name] = {
                    'duration': duration,
                    'tokens': 0,
                    'tokens_per_sec': 0.0,
                    'success': False
                }
                print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Å—Ü–µ–Ω–∞—Ä–∏–∏ {description}")
                print(f"‚è±Ô∏è  –í—Ä–µ–º—è –¥–æ –æ—à–∏–±–∫–∏: {format_duration(duration)}")
                results[scenario_name] = "–û—à–∏–±–∫–∞"
                
        except Exception as e:
            end_time = time.time()
            duration = end_time - start_time
            performance_stats[scenario_name] = {
                'duration': duration,
                'tokens': 0,
                'tokens_per_sec': 0.0,
                'success': False
            }
            print(f"‚ùå –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –≤ —Å—Ü–µ–Ω–∞—Ä–∏–∏ {description}: {e}")
            print(f"‚è±Ô∏è  –í—Ä–µ–º—è –¥–æ –æ—à–∏–±–∫–∏: {format_duration(duration)}")
            results[scenario_name] = f"–ò—Å–∫–ª—é—á–µ–Ω–∏–µ: {e}"
        
        print()
    
    # 8. –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\n" + "="*60)
    print("‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: llm_logs/test_comparison/{model_name}/")
    
    if scenario_choice == '5':
        print("\n–í—Å–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏:")
        print("  1. 1_without_reflection/ - –±—ã—Å—Ç—Ä–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è")
        print("  2. 2_with_reflection/ - —Å —Ä–µ—Ñ–ª–µ–∫—Å–∏–µ–π –∏ —É–ª—É—á—à–µ–Ω–∏–µ–º")
        print("  3. 3_with_cleaning/ - —Å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ—á–∏—Å—Ç–∫–æ–π")
        print("  4. 4_structured_analysis/ - —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
    else:
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –∑–∞–ø—É—â–µ–Ω–Ω—ã–π —Å—Ü–µ–Ω–∞—Ä–∏–π
        scenario_info = {
            '1': ("1_without_reflection", "–±—ã—Å—Ç—Ä–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è"),
            '2': ("2_with_reflection", "—Å —Ä–µ—Ñ–ª–µ–∫—Å–∏–µ–π –∏ —É–ª—É—á—à–µ–Ω–∏–µ–º"),
            '3': ("3_with_cleaning", "—Å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ—á–∏—Å—Ç–∫–æ–π"),
            '4': ("4_structured_analysis", "—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
        }
        scenario_name, description = scenario_info[scenario_choice]
        print(f"\n–ó–∞–ø—É—â–µ–Ω–Ω—ã–π —Å—Ü–µ–Ω–∞—Ä–∏–π:")
        print(f"  {scenario_name}/ - {description}")
    
    print("\n–°—Ç–∞—Ç—É—Å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:")
    for scenario, status in results.items():
        print(f"  {scenario}: {status}")
    
    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    if performance_stats:
        print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:")
        print("-" * 60)
        
        for scenario_name, stats in performance_stats.items():
            if stats['success']:
                print(f"  {scenario_name}:")
                print(f"    ‚è±Ô∏è  –í—Ä–µ–º—è: {format_duration(stats['duration'])}")
                print(f"    üî¢ –¢–æ–∫–µ–Ω–æ–≤: ~{stats['tokens']}")
                print(f"    üöÄ –°–∫–æ—Ä–æ—Å—Ç—å: {stats['tokens_per_sec']:.1f} —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫")
            else:
                print(f"  {scenario_name}: ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–º–µ—Ä–∏—Ç—å (–æ—à–∏–±–∫–∞)")
        
        # –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        if len(performance_stats) > 1:
            successful_stats = {k: v for k, v in performance_stats.items() if v['success']}
            if successful_stats:
                print("\nüèÜ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤:")
                fastest = max(successful_stats.items(), key=lambda x: x[1]['tokens_per_sec'])
                slowest = min(successful_stats.items(), key=lambda x: x[1]['tokens_per_sec'])
                
                print(f"  ü•á –°–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π: {fastest[0]} ({fastest[1]['tokens_per_sec']:.1f} —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫)")
                print(f"  üêå –°–∞–º—ã–π –º–µ–¥–ª–µ–Ω–Ω—ã–π: {slowest[0]} ({slowest[1]['tokens_per_sec']:.1f} —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫)")
                
                # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                total_duration = sum(s['duration'] for s in successful_stats.values())
                total_tokens = sum(s['tokens'] for s in successful_stats.values())
                avg_speed = total_tokens / total_duration if total_duration > 0 else 0
                
                print(f"  üìà –û–±—â–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:")
                print(f"    ‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è: {format_duration(total_duration)}")
                print(f"    üî¢ –û–±—â–µ–µ —Ç–æ–∫–µ–Ω–æ–≤: ~{total_tokens}")
                print(f"    üöÄ –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {avg_speed:.1f} —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫")
    
    print("="*60)

if __name__ == "__main__":
    asyncio.run(main())
