#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
"""

import asyncio
import json
import logging
import os
import select
import sys
import termios
import time
import tty

import aiohttp

from chat_analyzer import ChatAnalyzer
from database import DatabaseManager

os.environ['OLLAMA_BASE_URL'] = 'http://192.168.1.75:11434'

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

DEFAULT_ANIMATION_SPEED = 0.02
DEFAULT_DATE = "2025-10-16"
TOKENS_PER_CHAR = 4

def estimate_tokens(text: str) -> int:
    return len(text) // TOKENS_PER_CHAR

def calculate_tokens_per_second(response_text: str, duration: float) -> float:
    if duration <= 0:
        return 0.0
    tokens = estimate_tokens(response_text)
    return tokens / duration

def format_duration(seconds: float) -> str:
    if seconds < 60:
        return f"{seconds:.1f}—Å"
    elif seconds < 3600:
        minutes = int(seconds // 60)
        secs = seconds % 60
        return f"{minutes}–º {secs:.1f}—Å"
    else:
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = seconds % 60
        return f"{hours}—á {minutes}–º {secs:.1f}—Å"

def print_with_animation(text: str, prefix: str = "ü§ñ", delay: float = 0.02):
    print(f"\n{prefix} –ì–µ–Ω–µ—Ä–∞—Ü–∏—è:")
    print("-" * 50)
    
    if delay == 0:
        print(text)
        print("-" * 50)
        return
    
    print("üí° –ù–∞–∂–º–∏—Ç–µ –ª—é–±—É—é –∫–ª–∞–≤–∏—à—É –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∞–Ω–∏–º–∞—Ü–∏–∏")
    print("-" * 50)
    
    if len(text) > 1000 and delay > 0.01:
        print("‚ö†Ô∏è –î–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç, —É—Å–∫–æ—Ä—è–µ–º –∞–Ω–∏–º–∞—Ü–∏—é...")
        delay = min(delay, 0.01)
    
    for i, char in enumerate(text):
        print(char, end='', flush=True)
        
        if i % 10 == 0 and check_key_pressed():
            key = get_key()
            if key:
                print(f"\n‚ö° –ê–Ω–∏–º–∞—Ü–∏—è —É—Å–∫–æ—Ä–µ–Ω–∞! (–Ω–∞–∂–∞—Ç–∞ –∫–ª–∞–≤–∏—à–∞: {repr(key)})")
                delay = 0.001
        
        time.sleep(delay)
    
    print("\n" + "-" * 50)

def print_progress_stage(stage: str, description: str = ""):
    print(f"\nüîÑ {stage}")
    if description:
        print(f"   {description}")
    print("-" * 30)

def check_key_pressed():
    try:
        if sys.platform == 'win32':
            import msvcrt
            return msvcrt.kbhit()
        else:
            return select.select([sys.stdin], [], [], 0) == ([sys.stdin], [], [])
    except:
        return False

def get_key():
    try:
        if sys.platform == 'win32':
            import msvcrt
            return msvcrt.getch().decode('utf-8')
        else:
            return sys.stdin.read(1)
    except:
        return None

async def get_available_models(base_url: str) -> list:
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(f"{base_url}/api/tags", timeout=10) as response:
                if response.status == 200:
                    data = await response.json()
                    models = []
                    for model_info in data.get('models', []):
                        model_name = model_info.get('name', '')
                        if model_name:
                            models.append(model_name)
                    return models
                else:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: HTTP {response.status}")
                    return []
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama: {e}")
        return []

class ProgressChatAnalyzer(ChatAnalyzer):
    def __init__(self, config, show_generation=False, animation_speed=0.02):
        super().__init__(config)
        self.show_generation = show_generation
        self.animation_speed = animation_speed
    
    def _get_scenario_name(self, enable_reflection: bool, clean_data_first: bool) -> str:
        if clean_data_first:
            return "with_cleaning"
        elif enable_reflection:
            return "with_reflection"
        else:
            return "without_reflection"
    
    async def analyze_chat_with_specific_model(self, messages, provider_name, model_id, user_id=None, 
                                             enable_reflection=False, clean_data_first=False):
        if not self.show_generation:
            return await super().analyze_chat_with_specific_model(
                messages, provider_name, model_id, user_id, enable_reflection, clean_data_first
            )
        
        print_progress_stage("üìä –ê–Ω–∞–ª–∏–∑ —á–∞—Ç–∞", f"–ú–æ–¥–µ–ª—å: {model_id}")
        
        from llm_logger import LLMLogger
        
        scenario = self._get_scenario_name(enable_reflection, clean_data_first)
        test_mode = os.environ.get('TEST_MODE') == 'true'
        
        llm_logger = LLMLogger(
            scenario=scenario,
            model_name=model_id,
            test_mode=test_mode
        )
        llm_logger.set_session_info(provider_name, model_id, None, user_id)
        
        if provider_name == 'ollama' and 'ollama' in self.config:
            provider = self.provider_factory.create_provider(provider_name, self.config['ollama'])
        else:
            provider = self.provider_factory.create_provider(provider_name, self.config)
        if not provider:
            print("‚ùå –ü—Ä–æ–≤–∞–π–¥–µ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return None
        
        if hasattr(provider, 'set_model'):
            provider.set_model(model_id)
        
        if clean_data_first:
            print_progress_stage("üßπ –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö", "–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π...")
            
            temp_chat_context = {
                'total_messages': len(messages),
                'date': messages[0].get('message_time', 0) if messages else 0,
                'provider': provider_name,
                'model': model_id
            }
            
            messages = await self.clean_messages(provider, messages, temp_chat_context, llm_logger)
            if not messages:
                print("‚ùå –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ —É–¥–∞–ª–∞—Å—å –∏–ª–∏ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å —Å–æ–æ–±—â–µ–Ω–∏–π")
                return None
            print(f"‚úÖ –û—á–∏—â–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–π: {len(messages)}")
        
        print_progress_stage("üìù –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è", "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ä–µ–∑—é–º–µ...")
        summary = await provider.summarize_chat(messages)
        
        if self.show_generation and summary:
            print_with_animation(summary, "üìù –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è", self.animation_speed)
        
        if llm_logger and summary:
            llm_logger.log_raw_result(summary)
            llm_logger.log_formatted_result(summary)
            llm_logger.log_session_summary({
                'summary': summary,
                'messages_count': len(messages),
                'model': model_id,
                'provider': provider_name
            })
        
        if not enable_reflection:
            return summary
        
        print_progress_stage("ü§î –†–µ—Ñ–ª–µ–∫—Å–∏—è", "–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏...")
        reflection = await provider.generate_response(
            f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–∞—á–µ—Å—Ç–≤–æ —ç—Ç–æ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –∏ –ø—Ä–µ–¥–ª–æ–∂–∏ —É–ª—É—á—à–µ–Ω–∏—è:\n\n{summary}"
        )
        
        if self.show_generation and reflection:
            print_with_animation(reflection, "ü§î –†–µ—Ñ–ª–µ–∫—Å–∏—è", self.animation_speed)
        
        print_progress_stage("‚ú® –£–ª—É—á—à–µ–Ω–∏–µ", "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏...")
        improved = await provider.generate_response(
            f"–£–ª—É—á—à–∏ —ç—Ç—É —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞:\n\n–°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è:\n{summary}\n\n–ê–Ω–∞–ª–∏–∑:\n{reflection}"
        )
        
        if self.show_generation and improved:
            print_with_animation(improved, "‚ú® –£–ª—É—á—à–µ–Ω–∏–µ", self.animation_speed)
        
        return {
            'summary': summary,
            'reflection': reflection,
            'improved': improved
        }
    
    async def structured_analysis_with_specific_model(self, messages, provider_name, model_name, user_id):
        if not self.show_generation:
            return await super().structured_analysis_with_specific_model(
                messages, provider_name, model_name, user_id
            )
        
        print_progress_stage("üèóÔ∏è –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑", f"–ú–æ–¥–µ–ª—å: {model_name}")
        
        from llm_logger import LLMLogger
        
        test_mode = os.environ.get('TEST_MODE') == 'true'
        llm_logger = LLMLogger(
            scenario="structured_analysis",
            model_name=model_name,
            test_mode=test_mode
        )
        
        llm_logger.set_session_info(provider_name, model_name, None, user_id)
        
        if provider_name == 'ollama' and 'ollama' in self.config:
            provider = self.provider_factory.create_provider(provider_name, self.config['ollama'])
        else:
            provider = self.provider_factory.create_provider(provider_name, self.config)
        if not provider:
            print("‚ùå –ü—Ä–æ–≤–∞–π–¥–µ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return None
        
        if hasattr(provider, 'set_model'):
            provider.set_model(model_name)
        
        print_progress_stage("üè∑Ô∏è –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è", "–ê–Ω–∞–ª–∏–∑ —Ç–∏–ø–æ–≤ —Å–æ–æ–±—â–µ–Ω–∏–π...")
        
        def classification_callback(chunk):
            if self.show_generation:
                print(chunk, end='', flush=True)
        
        classification = await self._classify_messages(provider, messages, llm_logger, stream_callback=classification_callback)
        
        if self.show_generation and classification:
            print()
        
        relevant_messages = self._filter_by_classification(messages, classification)
        
        print(f"‚úÖ –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π: {len(relevant_messages)} –∏–∑ {len(messages)}")
        
        print_progress_stage("üîç –≠–∫—Å—Ç—Ä–∞–∫—Ü–∏—è —Å–ª–æ—Ç–æ–≤", "–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–æ–±—ã—Ç–∏–π –∏ —Ñ–∞–∫—Ç–æ–≤...")
        
        def extraction_callback(chunk):
            if self.show_generation:
                print(chunk, end='', flush=True)
        
        events = await self._extract_slots(provider, relevant_messages, classification, llm_logger, stream_callback=extraction_callback)
        
        if self.show_generation and events:
            print()
        
        print_progress_stage("üë®‚Äçüë©‚Äçüëß‚Äçüë¶ –°–≤–æ–¥–∫–∞ –¥–ª—è —Ä–æ–¥–∏—Ç–µ–ª–µ–π", "–°–æ–∑–¥–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞...")
        
        def summary_callback(chunk):
            if self.show_generation:
                print(chunk, end='', flush=True)
        
        summary = await self._generate_parent_summary(provider, events, llm_logger, stream_callback=summary_callback)
        
        if self.show_generation and summary:
            print()
        
        if llm_logger and summary:
            llm_logger.log_raw_result(summary)
            llm_logger.log_formatted_result(summary)
            llm_logger.log_session_summary({
                'summary': summary,
                'events': events,
                'classification': classification
            })
        
        return {
            'summary': summary,
            'events': events,
            'classification': classification
        }

def get_scenario_configs():
    return [
        ("1_without_reflection", False, False, False, "–ë–µ–∑ —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏"),
        ("2_with_reflection", True, False, False, "–° —Ä–µ—Ñ–ª–µ–∫—Å–∏–µ–π"),
        ("3_with_cleaning", True, True, False, "–° –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ—á–∏—Å—Ç–∫–æ–π"),
        ("4_structured_analysis", False, False, True, "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
    ]

def print_scenario_menu():
    print("–î–æ—Å—Ç—É–ø–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏:")
    print("  1. –ë–µ–∑ —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏ (–±—ã—Å—Ç—Ä–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è)")
    print("  2. –° —Ä–µ—Ñ–ª–µ–∫—Å–∏–µ–π (–∞–Ω–∞–ª–∏–∑ –∏ —É–ª—É—á—à–µ–Ω–∏–µ)")
    print("  3. –° –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ—á–∏—Å—Ç–∫–æ–π (—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è + —Ä–µ—Ñ–ª–µ–∫—Å–∏—è)")
    print("  4. –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è + —ç–∫—Å—Ç—Ä–∞–∫—Ü–∏—è + —Å–≤–æ–¥–∫–∞)")
    print("  5. –í—Å–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ (—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ)")

def select_scenarios(scenario_choice):
    all_scenarios = get_scenario_configs()
    
    if scenario_choice == '1':
        return [all_scenarios[0]]
    elif scenario_choice == '2':
        return [all_scenarios[1]]
    elif scenario_choice == '3':
        return [all_scenarios[2]]
    elif scenario_choice == '4':
        return [all_scenarios[3]]
    else:
        return all_scenarios

def calculate_performance_stats(result, duration, structured_analysis=False):
    if isinstance(result, dict):
        if structured_analysis:
            summary_text = result.get('summary', '') or ''
            events = result.get('events', [])
            classification = result.get('classification', [])
            
            events_text = str(events) if events else ''
            classification_text = str(classification) if classification else ''
            
            total_tokens = estimate_tokens(summary_text) + estimate_tokens(events_text) + estimate_tokens(classification_text)
            tokens_per_sec = calculate_tokens_per_second(summary_text + events_text + classification_text, duration)
        else:
            summary_text = result.get('summary', '') or ''
            reflection_text = result.get('reflection', '') or ''
            improved_text = result.get('improved', '') or ''
            
            total_tokens = estimate_tokens(summary_text) + estimate_tokens(reflection_text) + estimate_tokens(improved_text)
            tokens_per_sec = calculate_tokens_per_second(summary_text + reflection_text + improved_text, duration)
    else:
        result_text = str(result) if result else ''
        total_tokens = estimate_tokens(result_text)
        tokens_per_sec = calculate_tokens_per_second(result_text, duration)
    
    return total_tokens, tokens_per_sec

def print_performance_summary(performance_stats):
    if not performance_stats:
        return
    
    print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:")
    print("-" * 60)
    
    for scenario_name, stats in performance_stats.items():
        if stats['success']:
            print(f"  {scenario_name}:")
            print(f"    ‚è±Ô∏è  –í—Ä–µ–º—è: {format_duration(stats['duration'])}")
            print(f"    üî¢ –¢–æ–∫–µ–Ω–æ–≤: ~{stats['tokens']}")
            print(f"    üöÄ –°–∫–æ—Ä–æ—Å—Ç—å: {stats['tokens_per_sec']:.1f} —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫")
        else:
            print(f"  {scenario_name}: ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–º–µ—Ä–∏—Ç—å (–æ—à–∏–±–∫–∞)")
    
    if len(performance_stats) > 1:
        successful_stats = {k: v for k, v in performance_stats.items() if v['success']}
        if successful_stats:
            print("\nüèÜ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤:")
            fastest = max(successful_stats.items(), key=lambda x: x[1]['tokens_per_sec'])
            slowest = min(successful_stats.items(), key=lambda x: x[1]['tokens_per_sec'])
            
            print(f"  ü•á –°–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π: {fastest[0]} ({fastest[1]['tokens_per_sec']:.1f} —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫)")
            print(f"  üêå –°–∞–º—ã–π –º–µ–¥–ª–µ–Ω–Ω—ã–π: {slowest[0]} ({slowest[1]['tokens_per_sec']:.1f} —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫)")
            
            total_duration = sum(s['duration'] for s in successful_stats.values())
            total_tokens = sum(s['tokens'] for s in successful_stats.values())
            avg_speed = total_tokens / total_duration if total_duration > 0 else 0
            
            print(f"  üìà –û–±—â–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:")
            print(f"    ‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è: {format_duration(total_duration)}")
            print(f"    üî¢ –û–±—â–µ–µ —Ç–æ–∫–µ–Ω–æ–≤: ~{total_tokens}")
            print(f"    üöÄ –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {avg_speed:.1f} —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫")

async def main():
    print("ü§ñ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏")
    print("‚ú® –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:")
    print("   ‚Ä¢ –ü–æ–∫–∞–∑ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ (–≤–∫–ª—é—á–µ–Ω –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)")
    print("   ‚Ä¢ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –±—ã—Å—Ç—Ä—ã–π –≤—ã–≤–æ–¥ –±–µ–∑ –∑–∞–¥–µ—Ä–∂–µ–∫")
    print("   ‚Ä¢ –ü–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç Ollama API")
    print("   ‚Ä¢ –ü–æ–¥—Ä–æ–±–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å –ø–æ —ç—Ç–∞–ø–∞–º\n")
    
    base_url = os.environ.get('OLLAMA_BASE_URL', 'http://localhost:11434')
    print(f"üîç –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π —Å —Å–µ—Ä–≤–µ—Ä–∞ {base_url}...")
    
    available_models = await get_available_models(base_url)
    
    if not available_models:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –∏–ª–∏ —Å–µ—Ä–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
        print("üí° –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω –∏ –¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ –∞–¥—Ä–µ—Å—É:", base_url)
        return
    
    models = {}
    for i, model in enumerate(available_models, 1):
        models[str(i)] = model
    
    print(f"\n‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(available_models)} –º–æ–¥–µ–ª–µ–π:")
    for key, model in models.items():
        print(f"  {key}. {model}")
    
    choice = input(f"\n–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å (1-{len(available_models)}): ").strip()
    model_name = models.get(choice)
    
    if not model_name:
        print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä")
        return
    
    print(f"\n‚úÖ –í—ã–±—Ä–∞–Ω–∞ –º–æ–¥–µ–ª—å: {model_name}\n")
    
    print_scenario_menu()
    
    scenario_choice = input("\n–í—ã–±–µ—Ä–∏—Ç–µ —Å—Ü–µ–Ω–∞—Ä–∏–π (1-5): ").strip()
    
    if scenario_choice not in ['1', '2', '3', '4', '5']:
        print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä —Å—Ü–µ–Ω–∞—Ä–∏—è")
        return
    
    print(f"\n‚úÖ –í—ã–±—Ä–∞–Ω —Å—Ü–µ–Ω–∞—Ä–∏–π: {scenario_choice}\n")
    
    show_generation = True
    animation_speed = DEFAULT_ANIMATION_SPEED
    
    print("‚úÖ –ü–æ–∫–∞–∑ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤–∫–ª—é—á–µ–Ω –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
    print(f"‚úÖ –°–∫–æ—Ä–æ—Å—Ç—å –∞–Ω–∏–º–∞—Ü–∏–∏: {animation_speed}s –Ω–∞ —Å–∏–º–≤–æ–ª (–Ω–æ—Ä–º–∞–ª—å–Ω–æ)")
    print()
    
    try:
        db = DatabaseManager('bot_database.db')
        
        all_groups = db.get_all_groups()
        
        if not all_groups:
            print("‚ùå –ù–µ—Ç –≥—Ä—É–ø–ø –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö")
            return
        
        chat_info = all_groups[0]
        group_id = chat_info['group_id']
        chat_name = chat_info.get('group_name', 'Unknown')
        
        print(f"üì± –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≥—Ä—É–ø–ø–∞: {chat_name} (ID: {group_id})")
        
        vk_chats = db.get_group_vk_chats(group_id)
        if not vk_chats:
            print("‚ùå –ù–µ—Ç —Å–≤—è–∑–∞–Ω–Ω—ã—Ö VK —á–∞—Ç–æ–≤ –¥–ª—è —ç—Ç–æ–π –≥—Ä—É–ø–ø—ã")
            return
        
        vk_chat_id = vk_chats[0]['chat_id']
        print(f"üì± –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è VK —á–∞—Ç: {vk_chat_id}")
        
        messages = db.get_messages_by_date(vk_chat_id, DEFAULT_DATE)
        
        if not messages:
            print(f"‚ùå –ù–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–π –∑–∞ {DEFAULT_DATE}")
            return
        
        print(f"üìä –ù–∞–π–¥–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–π: {len(messages)}\n")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö: {e}")
        return
    
    os.environ['TEST_MODE'] = 'true'
    os.environ['TEST_MODEL_NAME'] = model_name
    os.environ['ENABLE_LLM_LOGGING'] = 'true'
    
    try:
        from config import AI_PROVIDERS
        analyzer = ProgressChatAnalyzer(AI_PROVIDERS, show_generation=show_generation, animation_speed=animation_speed)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞: {e}")
        return
    
    scenarios_to_run = select_scenarios(scenario_choice)
    results = {}
    performance_stats = {}
    
    for scenario_name, enable_reflection, clean_data_first, structured_analysis, description in scenarios_to_run:
        print(f"üîÑ –ó–∞–ø—É—Å–∫ —Å—Ü–µ–Ω–∞—Ä–∏—è: {description} ({scenario_name})...")
        
        start_time = time.time()
        
        try:
            if structured_analysis:
                result = await analyzer.structured_analysis_with_specific_model(
                    messages=messages,
                    provider_name="ollama",
                    model_name=model_name,
                    user_id=None
                )
            else:
                result = await analyzer.analyze_chat_with_specific_model(
                    messages=messages,
                    provider_name="ollama",
                    model_id=model_name,
                    user_id=None,
                    enable_reflection=enable_reflection,
                    clean_data_first=clean_data_first
                )
            
            end_time = time.time()
            duration = end_time - start_time
            
            if result:
                total_tokens, tokens_per_sec = calculate_performance_stats(result, duration, structured_analysis)
                
                performance_stats[scenario_name] = {
                    'duration': duration,
                    'tokens': total_tokens,
                    'tokens_per_sec': tokens_per_sec,
                    'success': True
                }
                
                print(f"‚úÖ –°—Ü–µ–Ω–∞—Ä–∏–π {description} –∑–∞–≤–µ—Ä—à–µ–Ω")
                print(f"‚è±Ô∏è  –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {format_duration(duration)}")
                print(f"üî¢ –¢–æ–∫–µ–Ω–æ–≤ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: ~{total_tokens}")
                print(f"üöÄ –°–∫–æ—Ä–æ—Å—Ç—å: {tokens_per_sec:.1f} —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫")
                results[scenario_name] = "–£—Å–ø–µ—à–Ω–æ"
            else:
                end_time = time.time()
                duration = end_time - start_time
                performance_stats[scenario_name] = {
                    'duration': duration,
                    'tokens': 0,
                    'tokens_per_sec': 0.0,
                    'success': False
                }
                print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Å—Ü–µ–Ω–∞—Ä–∏–∏ {description}")
                print(f"‚è±Ô∏è  –í—Ä–µ–º—è –¥–æ –æ—à–∏–±–∫–∏: {format_duration(duration)}")
                results[scenario_name] = "–û—à–∏–±–∫–∞"
                
        except Exception as e:
            end_time = time.time()
            duration = end_time - start_time
            performance_stats[scenario_name] = {
                'duration': duration,
                'tokens': 0,
                'tokens_per_sec': 0.0,
                'success': False
            }
            print(f"‚ùå –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –≤ —Å—Ü–µ–Ω–∞—Ä–∏–∏ {description}: {e}")
            print(f"‚è±Ô∏è  –í—Ä–µ–º—è –¥–æ –æ—à–∏–±–∫–∏: {format_duration(duration)}")
            results[scenario_name] = f"–ò—Å–∫–ª—é—á–µ–Ω–∏–µ: {e}"
        
        print()
    
    print("\n" + "="*60)
    print("‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: llm_logs/test_comparison/{model_name}/")
    
    if scenario_choice == '5':
        print("\n–í—Å–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏:")
        print("  1. 1_without_reflection/ - –±—ã—Å—Ç—Ä–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è")
        print("  2. 2_with_reflection/ - —Å —Ä–µ—Ñ–ª–µ–∫—Å–∏–µ–π –∏ —É–ª—É—á—à–µ–Ω–∏–µ–º")
        print("  3. 3_with_cleaning/ - —Å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ—á–∏—Å—Ç–∫–æ–π")
        print("  4. 4_structured_analysis/ - —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
    else:
        scenario_info = {
            '1': ("1_without_reflection", "–±—ã—Å—Ç—Ä–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è"),
            '2': ("2_with_reflection", "—Å —Ä–µ—Ñ–ª–µ–∫—Å–∏–µ–π –∏ —É–ª—É—á—à–µ–Ω–∏–µ–º"),
            '3': ("3_with_cleaning", "—Å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ—á–∏—Å—Ç–∫–æ–π"),
            '4': ("4_structured_analysis", "—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
        }
        scenario_name, description = scenario_info[scenario_choice]
        print(f"\n–ó–∞–ø—É—â–µ–Ω–Ω—ã–π —Å—Ü–µ–Ω–∞—Ä–∏–π:")
        print(f"  {scenario_name}/ - {description}")
    
    print("\n–°—Ç–∞—Ç—É—Å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:")
    for scenario, status in results.items():
        print(f"  {scenario}: {status}")
    
    print_performance_summary(performance_stats)
    print("="*60)

if __name__ == "__main__":
    asyncio.run(main())