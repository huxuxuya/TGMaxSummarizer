"""
OpenRouter AI –ø—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —á–∞—Ç–æ–≤
"""
import httpx
import json
from typing import List, Dict, Optional, Any
from .base_provider import BaseAIProvider

class OpenRouterProvider(BaseAIProvider):
    """–ü—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å OpenRouter API (DeepSeek –∏ –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏)"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.api_key = config.get('OPENROUTER_API_KEY', '')
        self.base_url = "https://openrouter.ai/api/v1"
        
        # –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ OpenRouter (–≤—Å–µ 51 –±–µ—Å–ø–ª–∞—Ç–Ω–∞—è –º–æ–¥–µ–ª—å, –Ω–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ–ø 10)
        self.available_models = {
            "alibaba/tongyi-deepresearch-30b-a3b:free": {
                "display_name": "Tongyi DeepResearch 30B A3B (free)",
                "description": "Tongyi DeepResearch is an agentic large language model developed by Tongyi Lab, with 30 billion tota...",
                "free": True,
                "context_length": 131072
            },
            "meituan/longcat-flash-chat:free": {
                "display_name": "Meituan: LongCat Flash Chat (free)",
                "description": "LongCat-Flash-Chat is a large-scale Mixture-of-Experts (MoE) model with 560B total parameters, of wh...",
                "free": True,
                "context_length": 131072
            },
            "nvidia/nemotron-nano-9b-v2:free": {
                "display_name": "NVIDIA: Nemotron Nano 9B V2 (free)",
                "description": "NVIDIA-Nemotron-Nano-9B-v2 is a large language model (LLM) trained from scratch by NVIDIA, and desig...",
                "free": True,
                "context_length": 128000
            },
            "deepseek/deepseek-chat-v3.1:free": {
                "display_name": "DeepSeek: DeepSeek V3.1 (free)",
                "description": "DeepSeek-V3...",
                "free": True,
                "context_length": 163800
            },
            "openai/gpt-oss-20b:free": {
                "display_name": "OpenAI: gpt-oss-20b (free)",
                "description": "gpt-oss-20b is an open-weight 21B parameter model released by OpenAI under the Apache 2...",
                "free": True,
                "context_length": 131072
            },
            "z-ai/glm-4.5-air:free": {
                "display_name": "Z.AI: GLM 4.5 Air (free)",
                "description": "GLM-4...",
                "free": True,
                "context_length": 131072
            },
            "qwen/qwen3-coder:free": {
                "display_name": "Qwen: Qwen3 Coder 480B A35B (free)",
                "description": "Qwen3-Coder-480B-A35B-Instruct is a Mixture-of-Experts (MoE) code generation model developed by the ...",
                "free": True,
                "context_length": 262144
            },
            "moonshotai/kimi-k2:free": {
                "display_name": "MoonshotAI: Kimi K2 0711 (free)",
                "description": "Kimi K2 Instruct is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, ...",
                "free": True,
                "context_length": 32768
            },
            "cognitivecomputations/dolphin-mistral-24b-venice-edition:free": {
                "display_name": "Venice: Uncensored (free)",
                "description": "Venice Uncensored Dolphin Mistral 24B Venice Edition is a fine-tuned variant of Mistral-Small-24B-In...",
                "free": True,
                "context_length": 32768
            },
            "google/gemma-3n-e2b-it:free": {
                "display_name": "Google: Gemma 3n 2B (free)",
                "description": "Gemma 3n E2B IT is a multimodal, instruction-tuned model developed by Google DeepMind, designed to o...",
                "free": True,
                "context_length": 8192
            },
            "tencent/hunyuan-a13b-instruct:free": {
                "display_name": "Tencent: Hunyuan A13B Instruct (free)",
                "description": "Hunyuan-A13B is a 13B active parameter Mixture-of-Experts (MoE) language model developed by Tencent,...",
                "free": True,
                "context_length": 32768
            },
            "tngtech/deepseek-r1t2-chimera:free": {
                "display_name": "TNG: DeepSeek R1T2 Chimera (free)",
                "description": "DeepSeek-TNG-R1T2-Chimera is the second-generation Chimera model from TNG Tech...",
                "free": True,
                "context_length": 163840
            },
            "mistralai/mistral-small-3.2-24b-instruct:free": {
                "display_name": "Mistral: Mistral Small 3.2 24B (free)",
                "description": "Mistral-Small-3...",
                "free": True,
                "context_length": 131072
            },
            "moonshotai/kimi-dev-72b:free": {
                "display_name": "MoonshotAI: Kimi Dev 72B (free)",
                "description": "Kimi-Dev-72B is an open-source large language model fine-tuned for software engineering and issue re...",
                "free": True,
                "context_length": 131072
            },
            "deepseek/deepseek-r1-0528-qwen3-8b:free": {
                "display_name": "DeepSeek: Deepseek R1 0528 Qwen3 8B (free)",
                "description": "DeepSeek-R1-0528 is a lightly upgraded release of DeepSeek R1 that taps more compute and smarter pos...",
                "free": True,
                "context_length": 131072
            },
            "deepseek/deepseek-r1-0528:free": {
                "display_name": "DeepSeek: R1 0528 (free)",
                "description": "May 28th update to the [original DeepSeek R1](/deepseek/deepseek-r1) Performance on par with [OpenAI...",
                "free": True,
                "context_length": 163840
            },
            "mistralai/devstral-small-2505:free": {
                "display_name": "Mistral: Devstral Small 2505 (free)",
                "description": "Devstral-Small-2505 is a 24B parameter agentic LLM fine-tuned from Mistral-Small-3...",
                "free": True,
                "context_length": 32768
            },
            "google/gemma-3n-e4b-it:free": {
                "display_name": "Google: Gemma 3n 4B (free)",
                "description": "Gemma 3n E4B-it is optimized for efficient execution on mobile and low-resource devices, such as pho...",
                "free": True,
                "context_length": 8192
            },
            "meta-llama/llama-3.3-8b-instruct:free": {
                "display_name": "Meta: Llama 3.3 8B Instruct (free)",
                "description": "A lightweight and ultra-fast variant of Llama 3...",
                "free": True,
                "context_length": 128000
            },
            "qwen/qwen3-4b:free": {
                "display_name": "Qwen: Qwen3 4B (free)",
                "description": "Qwen3-4B is a 4 billion parameter dense language model from the Qwen3 series, designed to support bo...",
                "free": True,
                "context_length": 40960
            },
            "qwen/qwen3-30b-a3b:free": {
                "display_name": "Qwen: Qwen3 30B A3B (free)",
                "description": "Qwen3, the latest generation in the Qwen large language model series, features both dense and mixtur...",
                "free": True,
                "context_length": 40960
            },
            "qwen/qwen3-8b:free": {
                "display_name": "Qwen: Qwen3 8B (free)",
                "description": "Qwen3-8B is a dense 8...",
                "free": True,
                "context_length": 40960
            },
            "qwen/qwen3-14b:free": {
                "display_name": "Qwen: Qwen3 14B (free)",
                "description": "Qwen3-14B is a dense 14...",
                "free": True,
                "context_length": 40960
            },
            "qwen/qwen3-235b-a22b:free": {
                "display_name": "Qwen: Qwen3 235B A22B (free)",
                "description": "Qwen3-235B-A22B is a 235B parameter mixture-of-experts (MoE) model developed by Qwen, activating 22B...",
                "free": True,
                "context_length": 131072
            },
            "tngtech/deepseek-r1t-chimera:free": {
                "display_name": "TNG: DeepSeek R1T Chimera (free)",
                "description": "DeepSeek-R1T-Chimera is created by merging DeepSeek-R1 and DeepSeek-V3 (0324), combining the reasoni...",
                "free": True,
                "context_length": 163840
            },
            "microsoft/mai-ds-r1:free": {
                "display_name": "Microsoft: MAI DS R1 (free)",
                "description": "MAI-DS-R1 is a post-trained variant of DeepSeek-R1 developed by the Microsoft AI team to improve the...",
                "free": True,
                "context_length": 163840
            },
            "shisa-ai/shisa-v2-llama3.3-70b:free": {
                "display_name": "Shisa AI: Shisa V2 Llama 3.3 70B  (free)",
                "description": "Shisa V2 Llama 3...",
                "free": True,
                "context_length": 32768
            },
            "arliai/qwq-32b-arliai-rpr-v1:free": {
                "display_name": "ArliAI: QwQ 32B RpR v1 (free)",
                "description": "QwQ-32B-ArliAI-RpR-v1 is a 32B parameter model fine-tuned from Qwen/QwQ-32B using a curated creative...",
                "free": True,
                "context_length": 32768
            },
            "agentica-org/deepcoder-14b-preview:free": {
                "display_name": "Agentica: Deepcoder 14B Preview (free)",
                "description": "DeepCoder-14B-Preview is a 14B parameter code generation model fine-tuned from DeepSeek-R1-Distill-Q...",
                "free": True,
                "context_length": 96000
            },
            "meta-llama/llama-4-maverick:free": {
                "display_name": "Meta: Llama 4 Maverick (free)",
                "description": "Llama 4 Maverick 17B Instruct (128E) is a high-capacity multimodal language model from Meta, built o...",
                "free": True,
                "context_length": 128000
            },
            "meta-llama/llama-4-scout:free": {
                "display_name": "Meta: Llama 4 Scout (free)",
                "description": "Llama 4 Scout 17B Instruct (16E) is a mixture-of-experts (MoE) language model developed by Meta, act...",
                "free": True,
                "context_length": 128000
            },
            "qwen/qwen2.5-vl-32b-instruct:free": {
                "display_name": "Qwen: Qwen2.5 VL 32B Instruct (free)",
                "description": "Qwen2...",
                "free": True,
                "context_length": 16384
            },
            "deepseek/deepseek-chat-v3-0324:free": {
                "display_name": "DeepSeek: DeepSeek V3 0324 (free)",
                "description": "DeepSeek V3, a 685B-parameter, mixture-of-experts model, is the latest iteration of the flagship cha...",
                "free": True,
                "context_length": 163840
            },
            "mistralai/mistral-small-3.1-24b-instruct:free": {
                "display_name": "Mistral: Mistral Small 3.1 24B (free)",
                "description": "Mistral Small 3...",
                "free": True,
                "context_length": 128000
            },
            "google/gemma-3-4b-it:free": {
                "display_name": "Google: Gemma 3 4B (free)",
                "description": "Gemma 3 introduces multimodality, supporting vision-language input and text outputs...",
                "free": True,
                "context_length": 32768
            },
            "google/gemma-3-12b-it:free": {
                "display_name": "Google: Gemma 3 12B (free)",
                "description": "Gemma 3 introduces multimodality, supporting vision-language input and text outputs...",
                "free": True,
                "context_length": 32768
            },
            "google/gemma-3-27b-it:free": {
                "display_name": "Google: Gemma 3 27B (free)",
                "description": "Gemma 3 introduces multimodality, supporting vision-language input and text outputs...",
                "free": True,
                "context_length": 96000
            },
            "nousresearch/deephermes-3-llama-3-8b-preview:free": {
                "display_name": "Nous: DeepHermes 3 Llama 3 8B Preview (free)",
                "description": "DeepHermes 3 Preview is the latest version of our flagship Hermes series of LLMs by Nous Research, a...",
                "free": True,
                "context_length": 131072
            },
            "cognitivecomputations/dolphin3.0-mistral-24b:free": {
                "display_name": "Dolphin3.0 Mistral 24B (free)",
                "description": "Dolphin 3...",
                "free": True,
                "context_length": 32768
            },
            "qwen/qwen2.5-vl-72b-instruct:free": {
                "display_name": "Qwen: Qwen2.5 VL 72B Instruct (free)",
                "description": "Qwen2...",
                "free": True,
                "context_length": 131072
            },
            "mistralai/mistral-small-24b-instruct-2501:free": {
                "display_name": "Mistral: Mistral Small 3 (free)",
                "description": "Mistral Small 3 is a 24B-parameter language model optimized for low-latency performance across commo...",
                "free": True,
                "context_length": 32768
            },
            "deepseek/deepseek-r1-distill-llama-70b:free": {
                "display_name": "DeepSeek: R1 Distill Llama 70B (free)",
                "description": "DeepSeek R1 Distill Llama 70B is a distilled large language model based on [Llama-3...",
                "free": True,
                "context_length": 8192
            },
            "deepseek/deepseek-r1:free": {
                "display_name": "DeepSeek: R1 (free)",
                "description": "DeepSeek R1 is here: Performance on par with [OpenAI o1](/openai/o1), but open-sourced and with full...",
                "free": True,
                "context_length": 163840
            },
            "google/gemini-2.0-flash-exp:free": {
                "display_name": "Google: Gemini 2.0 Flash Experimental (free)",
                "description": "Gemini Flash 2...",
                "free": True,
                "context_length": 1048576
            },
            "meta-llama/llama-3.3-70b-instruct:free": {
                "display_name": "Meta: Llama 3.3 70B Instruct (free)",
                "description": "The Meta Llama 3...",
                "free": True,
                "context_length": 65536
            },
            "qwen/qwen-2.5-coder-32b-instruct:free": {
                "display_name": "Qwen2.5 Coder 32B Instruct (free)",
                "description": "Qwen2...",
                "free": True,
                "context_length": 32768
            },
            "meta-llama/llama-3.2-3b-instruct:free": {
                "display_name": "Meta: Llama 3.2 3B Instruct (free)",
                "description": "Llama 3...",
                "free": True,
                "context_length": 131072
            },
            "qwen/qwen-2.5-72b-instruct:free": {
                "display_name": "Qwen2.5 72B Instruct (free)",
                "description": "Qwen2...",
                "free": True,
                "context_length": 32768
            },
            "mistralai/mistral-nemo:free": {
                "display_name": "Mistral: Mistral Nemo (free)",
                "description": "A 12B parameter model with a 128k token context length built by Mistral in collaboration with NVIDIA...",
                "free": True,
                "context_length": 131072
            },
            "google/gemma-2-9b-it:free": {
                "display_name": "Google: Gemma 2 9B (free)",
                "description": "Gemma 2 9B by Google is an advanced, open-source language model that sets a new standard for efficie...",
                "free": True,
                "context_length": 8192
            },
            "mistralai/mistral-7b-instruct:free": {
                "display_name": "Mistral: Mistral 7B Instruct (free)",
                "description": "A high-performing, industry-standard 7...",
                "free": True,
                "context_length": 32768
            },
        }

        
        # –ú–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (–±–µ—Å–ø–ª–∞—Ç–Ω–∞—è)
        self.default_model = "nvidia/nemotron-nano-9b-v2:free"
        self.current_model = self.default_model
        self.client = None
        
        # –ö—ç—à –¥–ª—è –º–æ–¥–µ–ª–µ–π (TTL: 1 —á–∞—Å)
        self._models_cache = None
        self._models_cache_timestamp = 0
        self._models_cache_ttl = 3600  # 1 —á–∞—Å –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        
        if self.api_key and self.api_key != 'your_openrouter_key':
            self.client = httpx.AsyncClient(
                timeout=30.0,
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json",
                    "HTTP-Referer": "https://github.com/your-repo",
                    "X-Title": "VK MAX Telegram Bot"
                }
            )
    
    async def initialize(self) -> bool:
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OpenRouter –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
        
        Returns:
            True –µ—Å–ª–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞, False –∏–Ω–∞—á–µ
        """
        try:
            if not self.api_key or self.api_key == 'your_openrouter_key':
                self.logger.error("‚ùå OpenRouter API –∫–ª—é—á –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
                return False
            
            if not self.client:
                self.logger.error("‚ùå –ö–ª–∏–µ–Ω—Ç OpenRouter –Ω–µ —Å–æ–∑–¥–∞–Ω")
                return False
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
            if not await self.is_available():
                self.logger.error("‚ùå OpenRouter –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
                return False
            
            self.is_initialized = True
            self.logger.info(f"‚úÖ OpenRouter –ø—Ä–æ–≤–∞–π–¥–µ—Ä —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –º–æ–¥–µ–ª—å—é: {self.current_model}")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ OpenRouter: {e}")
            return False
    
    async def summarize_chat(self, messages: List[Dict], chat_context: Optional[Dict] = None) -> str:
        """
        –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —á–∞—Ç–∞ —Å –ø–æ–º–æ—â—å—é OpenRouter
        
        Args:
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
            chat_context: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —á–∞—Ç–∞
            
        Returns:
            –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –≤ –≤–∏–¥–µ —Å—Ç—Ä–æ–∫–∏
        """
        try:
            if not self.client:
                return "‚ùå OpenRouter –∫–ª–∏–µ–Ω—Ç –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω"
            
            # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
            self.logger.info("üîß –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è OpenRouter...")
            optimized_messages = self.optimize_text(messages)
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            formatted_text = self.format_messages_for_analysis(optimized_messages)
            
            self.logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è OpenRouter:")
            self.logger.info(f"   –í—Å–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π: {len(messages)}")
            self.logger.info(f"   –ü–æ—Å–ª–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {len(optimized_messages)}")
            self.logger.info(f"   –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {len(formatted_text)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –í—ã–∑—ã–≤–∞–µ–º OpenRouter API
            self.logger.info("ü§ñ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –≤ OpenRouter...")
            
            # –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –µ—Å–ª–∏ –ª–æ–≥–≥–µ—Ä —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
            if self.llm_logger:
                self.llm_logger.log_llm_request(formatted_text, "summarization")
            
            summary = await self._call_openrouter_api(formatted_text)
            
            # –õ–æ–≥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –µ—Å–ª–∏ –ª–æ–≥–≥–µ—Ä —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
            if self.llm_logger and summary:
                self.llm_logger.log_llm_response(summary, "summarization")
            
            if summary:
                self.logger.info("‚úÖ –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∞ –æ—Ç OpenRouter")
                return summary
            else:
                self.logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—é–º–µ –æ—Ç OpenRouter")
                return "‚ùå –û—à–∏–±–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ OpenRouter"
                
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ OpenRouter: {e}")
            return f"‚ùå –û—à–∏–±–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏: {str(e)}"
    
    async def is_available(self) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å OpenRouter
        
        Returns:
            True –µ—Å–ª–∏ OpenRouter –¥–æ—Å—Ç—É–ø–µ–Ω, False –∏–Ω–∞—á–µ
        """
        if not self.validate_config():
            return False
        
        try:
            if not self.client:
                return False
            
            # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç API
            test_data = {
                "model": self.current_model,
                "messages": [{"role": "user", "content": "Hello"}],
                "max_tokens": 5
            }
            
            response = await self.client.post(
                f"{self.base_url}/chat/completions",
                json=test_data
            )
            
            return response.status_code == 200
            
        except Exception as e:
            self.logger.error(f"‚ùå OpenRouter –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
            return False
    
    async def generate_response(self, prompt: str) -> Optional[str]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å retry –ª–æ–≥–∏–∫–æ–π –¥–ª—è 429 –æ—à–∏–±–æ–∫
        
        Args:
            prompt: –¢–µ–∫—Å—Ç –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞
            
        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        import asyncio
        
        max_attempts = 4
        delay_seconds = 5
        
        for attempt in range(1, max_attempts + 1):
            try:
                if not self.client:
                    self.logger.error("‚ùå –ö–ª–∏–µ–Ω—Ç OpenRouter –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
                    return None
                
                if attempt > 1:
                    self.logger.info(f"üîÑ –ü–æ–ø—ã—Ç–∫–∞ {attempt}/{max_attempts} –ø–æ—Å–ª–µ –æ–∂–∏–¥–∞–Ω–∏—è {delay_seconds}—Å")
                else:
                    self.logger.info(f"ü§ñ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ OpenRouter –Ω–∞ –ø—Ä–æ–º–ø—Ç –¥–ª–∏–Ω–æ–π {len(prompt)} —Å–∏–º–≤–æ–ª–æ–≤")
                    self.logger.debug(f"=== GENERATE_RESPONSE INPUT ===")
                    self.logger.debug(f"Prompt length: {len(prompt)}")
                    self.logger.debug(f"Prompt preview: {prompt[:200]}...")
                    self.logger.debug(f"=== END INPUT ===")
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –µ—Å–ª–∏ –ª–æ–≥–≥–µ—Ä —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω (—Ç–æ–ª—å–∫–æ –Ω–∞ –ø–µ—Ä–≤–æ–π –ø–æ–ø—ã—Ç–∫–µ)
                    if self.llm_logger:
                        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É –ø—Ä–æ–º–ø—Ç–∞
                        request_type = "reflection" if "—Ä–µ—Ñ–ª–µ–∫—Å–∏—è" in prompt.lower() or "–∞–Ω–∞–ª–∏–∑" in prompt.lower() else "improvement"
                        self.llm_logger.log_llm_request(prompt, request_type)
                
                data = {
                    "model": self.current_model,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": 2000,
                    "temperature": 0.3
                }
                
                response = await self.client.post(
                    f"{self.base_url}/chat/completions",
                    json=data
                )
                
                if response.status_code == 200:
                    result = response.json()
                    if result.get("choices") and result["choices"][0].get("message", {}).get("content"):
                        content = result["choices"][0]["message"]["content"]
                        self.logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç OpenRouter –¥–ª–∏–Ω–æ–π {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
                        self.logger.debug(f"=== GENERATE_RESPONSE OUTPUT ===")
                        self.logger.debug(f"Response length: {len(content)}")
                        self.logger.debug(f"Response preview: {content[:200]}...")
                        self.logger.debug(f"=== END OUTPUT ===")
                        
                        # –õ–æ–≥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –µ—Å–ª–∏ –ª–æ–≥–≥–µ—Ä —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
                        if self.llm_logger:
                            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –æ—Ç–≤–µ—Ç–∞ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É –ø—Ä–æ–º–ø—Ç–∞
                            request_type = "reflection" if "—Ä–µ—Ñ–ª–µ–∫—Å–∏—è" in prompt.lower() or "–∞–Ω–∞–ª–∏–∑" in prompt.lower() else "improvement"
                            self.llm_logger.log_llm_response(content, request_type)
                        
                        return content
                    else:
                        self.logger.warning("‚ö†Ô∏è OpenRouter –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç")
                        return None
                elif response.status_code == 429:
                    # Rate limit - retry with delay
                    self.logger.warning(f"‚ö†Ô∏è Rate limit (429) - –ø–æ–ø—ã—Ç–∫–∞ {attempt}/{max_attempts}")
                    if attempt < max_attempts:
                        self.logger.info(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ {delay_seconds} —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–æ–º...")
                        await asyncio.sleep(delay_seconds)
                        continue  # Retry
                    else:
                        self.logger.error(f"‚ùå –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –ø–æ–ø—ã—Ç–æ–∫ ({max_attempts}), rate limit –Ω–µ —Å–Ω—è—Ç")
                        return None
                else:
                    # Other errors - fail immediately
                    self.logger.error(f"‚ùå OpenRouter –≤–µ—Ä–Ω—É–ª –æ—à–∏–±–∫—É: {response.status_code}")
                    return None
                    
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ OpenRouter: {e}")
                return None
        
        return None
    
    def get_provider_info(self) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–µ OpenRouter
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–µ
        """
        return {
            'name': 'OpenRouter',
            'display_name': 'OpenRouter (Multiple Models)',
            'description': 'OpenRouter —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —á–∞—Ç–æ–≤',
            'version': self.current_model,
            'current_model': self.current_model,
            'available_models': self.available_models,
            'max_tokens': 4000,
            'supports_streaming': True,
            'api_endpoint': self.base_url,
            'provider_type': 'openrouter'
        }
    
    def validate_config(self) -> bool:
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ OpenRouter
        
        Returns:
            True –µ—Å–ª–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≤–∞–ª–∏–¥–Ω–∞, False –∏–Ω–∞—á–µ
        """
        if not self.api_key or self.api_key == 'your_openrouter_key':
            self.logger.error("‚ùå OpenRouter API –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
            return False
        
        if len(self.api_key) < 20:
            self.logger.error("‚ùå OpenRouter API –∫–ª—é—á —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π")
            return False
        
        return True
    
    async def _call_openrouter_api(self, text: str) -> Optional[str]:
        """
        –í—ã–∑–≤–∞—Ç—å OpenRouter API –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            prompt = f"""–†–û–õ–¨: –ö–ª–∞—Å—Å–Ω—ã–π —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å 1 –∫–ª–∞—Å—Å–∞, —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç –≤–µ—á–µ—Ä–Ω—é—é —Å–≤–æ–¥–∫—É –¥–ª—è —Ä–æ–¥–∏—Ç–µ–ª–µ–π.

–ó–ê–î–ê–ß–ê: –°–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É –≤–∞–∂–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π –∏–∑ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ —á–∞—Ç–∞.

–í–ö–õ–Æ–ß–ê–ô:
- –°–æ–±—ã—Ç–∏—è —Å –¥–µ–¥–ª–∞–π–Ω–∞–º–∏
- –ù–æ–≤—ã–µ –ø—Ä–∞–≤–∏–ª–∞/—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
- –ú–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è
- –í–∞–∂–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
- –°—Å—ã–ª–∫–∏ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã

–ò–ì–ù–û–†–ò–†–£–ô: –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏—é –≤—Å—Ç—Ä–µ—á, –±—ã—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã, –ø—É—Å—Ç—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è.

–ü–†–ò–ú–ï–†–´ –•–û–†–û–®–ò–• –ü–£–ù–ö–¢–û–í:
‚úÖ "–ó–∞–≤—Ç—Ä–∞ –¥–æ 10:00 –ø—Ä–∏–Ω–µ—Å—Ç–∏ —Å–ø—Ä–∞–≤–∫—É –æ –∑–¥–æ—Ä–æ–≤—å–µ"
‚úÖ "–†–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–µ —Å–æ–±—Ä–∞–Ω–∏–µ 15 –æ–∫—Ç—è–±—Ä—è –≤ 18:00"
‚úÖ "–ó–∞–ø–æ–ª–Ω–∏—Ç—å –∞–Ω–∫–µ—Ç—É –ø–æ —Å—Å—ã–ª–∫–µ: https://..."

–ü–†–ò–ú–ï–†–´ –ü–õ–û–•–ò–• –ü–£–ù–ö–¢–û–í:
‚ùå "–ö—Ç–æ-—Ç–æ –∑–∞–±–∏—Ä–∞–µ—Ç —Ä–µ–±–µ–Ω–∫–∞"
‚ùå "–û–±—Å—É–∂–¥–µ–Ω–∏–µ –æ–±–µ–¥–∞"
‚ùå "–Ø —Ç–æ–∂–µ —Å–æ–≥–ª–∞—Å–µ–Ω"

–ß–∞—Ç:
{text}

–§–û–†–ú–ê–¢:
## üìã –í–ê–ñ–ù–´–ï –°–û–ë–´–¢–ò–Ø
## üö® –¢–†–ï–ë–£–ï–¢–°–Ø –î–ï–ô–°–¢–í–ò–ô

–ü–†–ê–í–ò–õ–ê: –ú–∞–∫—Å–∏–º—É–º 3-4 –ø—É–Ω–∫—Ç–∞ –≤ –∫–∞–∂–¥–æ–º —Ä–∞–∑–¥–µ–ª–µ. –¢–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã –∏ –¥–µ–π—Å—Ç–≤–∏—è.

–ü–†–ê–í–ò–õ–ê –§–û–†–ú–ê–¢–ò–†–û–í–ê–ù–ò–Ø:
- –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π HTML —Ç–µ–≥–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä <br>, <b>, <i>, <table>)
- –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π Markdown —Ç–∞–±–ª–∏—Ü—ã —Å —Å–∏–º–≤–æ–ª–∞–º–∏ |
- –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –ø—Ä–æ—Å—Ç—ã–µ —Å–ø–∏—Å–∫–∏ —Å –¥–µ—Ñ–∏—Å–∞–º–∏ (-)
- –ò—Å–ø–æ–ª—å–∑—É–π **–∂–∏—Ä–Ω—ã–π —Ç–µ–∫—Å—Ç** –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è –≤–∞–∂–Ω–æ–≥–æ
- –ò—Å–ø–æ–ª—å–∑—É–π –æ–±—ã—á–Ω—ã–µ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ –≤–º–µ—Å—Ç–æ <br>
- –ò—Å–ø–æ–ª—å–∑—É–π ## –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —Ä–∞–∑–¥–µ–ª–æ–≤"""

            data = {
                "model": self.current_model,
                "messages": [
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.7,
                "max_tokens": 1000
            }
            
            self.logger.info(f"üîó –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –≤ OpenRouter")
            self.logger.info(f"üìù –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
            self.logger.info(f"ü§ñ –ú–æ–¥–µ–ª—å: {self.current_model}")
            
            response = await self.client.post(
                f"{self.base_url}/chat/completions",
                json=data
            )
            
            self.logger.info(f"üì° –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç OpenRouter: {response.status_code}")
            
            if response.status_code == 200:
                result = response.json()
                
                if 'choices' in result and len(result['choices']) > 0:
                    return result['choices'][0]['message']['content']
                else:
                    self.logger.error("‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç OpenRouter")
                    self.logger.error(f"üìã –ü–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç: {result}")
                    return None
            else:
                self.logger.error(f"‚ùå HTTP –æ—à–∏–±–∫–∞ OpenRouter: {response.status_code}")
                self.logger.error(f"üìã –û—Ç–≤–µ—Ç —Å–µ—Ä–≤–µ—Ä–∞: {response.text}")
                return None
                
        except httpx.TimeoutException:
            self.logger.error("‚ùå –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –∫ OpenRouter")
            return None
        except httpx.HTTPStatusError as e:
            self.logger.error(f"‚ùå HTTP –æ—à–∏–±–∫–∞ OpenRouter: {e}")
            return None
        except Exception as e:
            self.logger.error(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ OpenRouter: {e}")
            return None
    
    async def __aenter__(self):
        """Async context manager entry"""
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        if self.client:
            await self.client.aclose()
    
    async def get_available_models(self) -> Dict[str, Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (—Ç–æ–ø 10 –ª—É—á—à–∏—Ö –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö —Å OpenRouter API)
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –¥–æ—Å—Ç—É–ø–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
        """
        import time
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
            current_time = time.time()
            if (self._models_cache is not None and 
                current_time - self._models_cache_timestamp < self._models_cache_ttl):
                self.logger.info("üìã –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π")
                return self._models_cache
            
            # –ü–æ–ª—É—á–∞–µ–º –∞–∫—Ç—É–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –æ—Ç OpenRouter API
            api_models = await self._fetch_models_from_api()
            
            if api_models:
                # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ø 10 –ª—É—á—à–∏—Ö –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
                top_10_models = self._select_top_10_free_models(api_models)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
                self._models_cache = top_10_models
                self._models_cache_timestamp = current_time
                
                return top_10_models
            else:
                # Fallback –∫ –ª–æ–∫–∞–ª—å–Ω–æ–º—É —Å–ø–∏—Å–∫—É, –µ—Å–ª–∏ API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
                self.logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –º–æ–¥–µ–ª–∏ –æ—Ç API, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫")
                fallback_models = self._get_fallback_models()
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º fallback –≤ –∫—ç—à –Ω–∞ –∫–æ—Ä–æ—Ç–∫–æ–µ –≤—Ä–µ–º—è (5 –º–∏–Ω—É—Ç)
                self._models_cache = fallback_models
                self._models_cache_timestamp = current_time
                self._models_cache_ttl = 300  # 5 –º–∏–Ω—É—Ç –¥–ª—è fallback
                
                return fallback_models
                
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –æ—Ç API: {e}")
            # Fallback –∫ –ª–æ–∫–∞–ª—å–Ω–æ–º—É —Å–ø–∏—Å–∫—É
            fallback_models = self._get_fallback_models()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º fallback –≤ –∫—ç—à –Ω–∞ –∫–æ—Ä–æ—Ç–∫–æ–µ –≤—Ä–µ–º—è
            self._models_cache = fallback_models
            self._models_cache_timestamp = time.time()
            self._models_cache_ttl = 300  # 5 –º–∏–Ω—É—Ç –¥–ª—è fallback
            
            return fallback_models
    
    async def _fetch_models_from_api(self) -> Optional[List[Dict[str, Any]]]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –æ—Ç OpenRouter API
        
        Returns:
            –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –æ—Ç API –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            if not self.client:
                self.logger.error("‚ùå –ö–ª–∏–µ–Ω—Ç OpenRouter –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
                return None
            
            self.logger.info("üîç –ü–æ–ª—É—á–∞–µ–º –∞–∫—Ç—É–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –æ—Ç OpenRouter API...")
            
            response = await self.client.get(f"{self.base_url}/models")
            
            if response.status_code == 200:
                data = response.json()
                models = data.get("data", [])
                self.logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(models)} –º–æ–¥–µ–ª–µ–π –æ—Ç OpenRouter API")
                return models
            else:
                self.logger.error(f"‚ùå OpenRouter API –≤–µ—Ä–Ω—É–ª –æ—à–∏–±–∫—É: {response.status_code}")
                return None
                
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ OpenRouter API: {e}")
            return None
    
    def _select_top_10_free_models(self, api_models: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
        """
        –í—ã–±—Ä–∞—Ç—å —Ç–æ–ø 10 –ª—É—á—à–∏—Ö –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏–∑ —Å–ø–∏—Å–∫–∞ API
        
        Args:
            api_models: –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –æ—Ç OpenRouter API
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ç–æ–ø 10 –º–æ–¥–µ–ª—è–º–∏
        """
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏
        free_models = []
        for model in api_models:
            model_id = model.get("id", "")
            pricing = model.get("pricing", {})
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å –±–µ—Å–ø–ª–∞—Ç–Ω–∞—è
            if pricing.get("prompt") == "0" and pricing.get("completion") == "0":
                # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏
                model_info = {
                    "display_name": model.get("name", model_id),
                    "description": model.get("description", "–ù–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è"),
                    "free": True,
                    "context_length": model.get("context_length", 4096),
                    "pricing": pricing,
                    "top_provider": model.get("top_provider", {}),
                    "per_request_limits": model.get("per_request_limits", {})
                }
                free_models.append((model_id, model_info))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É (—Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å –∏ —Ç.–¥.)
        def model_priority(item):
            model_id, model_info = item
            context_length = model_info.get("context_length", 0)
            
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Ö–æ—Ä–æ—à–∏—Ö –º–æ–¥–µ–ª–µ–π
            priority_models = {
                "deepseek/deepseek-chat-v3.1:free": 1000,
                "deepseek/deepseek-r1:free": 950,
                "deepseek/deepseek-v3:free": 900,
                "mistral/mistral-small-3.2:free": 850,
                "meta/llama-3.3-8b-instruct:free": 800,
                "qwen/qwen3-8b:free": 750,
                "google/gemma-3-12b:free": 700,
                "moonshotai/kimi-dev-72b:free": 650,
                "microsoft/mai-ds-r1:free": 600,
                "tng/deepseek-r1t-chimera:free": 550
            }
            
            # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –≤ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–æ–º —Å–ø–∏—Å–∫–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
            if model_id in priority_models:
                return priority_models[model_id]
            
            # –ò–Ω–∞—á–µ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–∞–∑–º–µ—Ä—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            return context_length
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É (–ø–æ —É–±—ã–≤–∞–Ω–∏—é)
        free_models.sort(key=model_priority, reverse=True)
        
        # –ë–µ—Ä–µ–º —Ç–æ–ø 10
        top_10 = free_models[:10]
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ª–æ–≤–∞—Ä—å
        result = {}
        for model_id, model_info in top_10:
            result[model_id] = model_info
        
        self.logger.info(f"‚úÖ –í—ã–±—Ä–∞–Ω–æ {len(result)} –ª—É—á—à–∏—Ö –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")
        return result
    
    def _get_fallback_models(self) -> Dict[str, Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–∏—Ç—å fallback —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π (–ª–æ–∫–∞–ª—å–Ω—ã–π)
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
        """
        # –¢–æ–ø 10 –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π OpenRouter (fallback —Å–ø–∏—Å–æ–∫)
        top_10_free_models = [
            "deepseek/deepseek-chat-v3.1:free",  # –õ—É—á—à–∞—è –±–µ—Å–ø–ª–∞—Ç–Ω–∞—è –º–æ–¥–µ–ª—å
            "deepseek/deepseek-r1:free",  # R1 - –æ—Ç–ª–∏—á–Ω–∞—è –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π
            "deepseek/deepseek-v3:free",  # V3 - —Ñ–ª–∞–≥–º–∞–Ω—Å–∫–∞—è –º–æ–¥–µ–ª—å
            "mistral/mistral-small-3.2:free",  # Mistral Small 3.2
            "meta/llama-3.3-8b-instruct:free",  # Llama 3.3 8B
            "qwen/qwen3-8b:free",  # Qwen3 8B
            "google/gemma-3-12b:free",  # Gemma 3 12B
            "moonshotai/kimi-dev-72b:free",  # Kimi Dev 72B
            "microsoft/mai-ds-r1:free",  # Microsoft MAI DS R1
            "tng/deepseek-r1t-chimera:free"  # DeepSeek R1T Chimera
        ]
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ–ø 10 –º–æ–¥–µ–ª–µ–π
        filtered_models = {}
        for model_id in top_10_free_models:
            if model_id in self.available_models:
                filtered_models[model_id] = self.available_models[model_id]
        
        return filtered_models
    
    def clear_models_cache(self):
        """
        –û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à –º–æ–¥–µ–ª–µ–π (–ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ)
        """
        self._models_cache = None
        self._models_cache_timestamp = 0
        self.logger.info("üóëÔ∏è –ö—ç—à –º–æ–¥–µ–ª–µ–π –æ—á–∏—â–µ–Ω")
    
    def set_model(self, model_id: str) -> bool:
        """
        –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        
        Args:
            model_id: ID –º–æ–¥–µ–ª–∏ –¥–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏
            
        Returns:
            True –µ—Å–ª–∏ –º–æ–¥–µ–ª—å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞, False –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–æ–¥–µ–ª—å –≤ –ø–æ–ª–Ω–æ–º —Å–ø–∏—Å–∫–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        if model_id in self.available_models:
            self.current_model = model_id
            self.logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å OpenRouter –∏–∑–º–µ–Ω–µ–Ω–∞ –Ω–∞: {model_id}")
            return True
        else:
            # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ, –Ω–æ –≤—Å–µ —Ä–∞–≤–Ω–æ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å
            # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –≤ OpenRouter, –Ω–æ –Ω–µ –≤ –Ω–∞—à–µ–º —Å–ø–∏—Å–∫–µ
            self.logger.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {model_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–º —Å–ø–∏—Å–∫–µ, –Ω–æ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –µ—ë")
            self.current_model = model_id
            self.logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å OpenRouter —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞: {model_id}")
            return True
    
    def get_current_model(self) -> str:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å
        
        Returns:
            ID —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏
        """
        return self.current_model
    
    def get_current_model_info(self) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏
        
        Returns:
            –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏
        """
        return self.available_models.get(self.current_model, {})
    
    def add_model(self, model_id: str, display_name: str, description: str, free: bool = False) -> bool:
        """
        –î–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –≤ —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö
        
        Args:
            model_id: ID –º–æ–¥–µ–ª–∏
            display_name: –û—Ç–æ–±—Ä–∞–∂–∞–µ–º–æ–µ –∏–º—è
            description: –û–ø–∏—Å–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
            free: –ë–µ—Å–ø–ª–∞—Ç–Ω–∞—è –ª–∏ –º–æ–¥–µ–ª—å
            
        Returns:
            True –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –¥–æ–±–∞–≤–ª–µ–Ω–∞
        """
        self.available_models[model_id] = {
            "display_name": display_name,
            "description": description,
            "free": free
        }
        self.logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å OpenRouter: {model_id}")
        return True
