"""
Ollama AI –ø—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
"""
import aiohttp
import json
from typing import List, Dict, Optional, Any
from .base_provider import BaseAIProvider

class OllamaProvider(BaseAIProvider):
    """–ü—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ —á–µ—Ä–µ–∑ Ollama API"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.base_url = config.get('base_url', 'http://localhost:11434')
        self.model = config.get('model', 'deepseek-r1:8b')
        self.timeout = config.get('timeout', 300)  # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π —Ç–∞–π–º–∞—É—Ç –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (5 –º–∏–Ω—É—Ç)
    
    def set_model(self, model_name: str):
        """
        –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        
        Args:
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'gemma3:12b')
        """
        self.model = model_name
        self.logger.info(f"üîó –ú–æ–¥–µ–ª—å Ollama –∏–∑–º–µ–Ω–µ–Ω–∞ –Ω–∞: {model_name}")
        
    async def summarize_chat(self, messages: List[Dict], chat_context: Optional[Dict] = None) -> str:
        """
        –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —á–∞—Ç–∞ —Å –ø–æ–º–æ—â—å—é Ollama
        
        Args:
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
            chat_context: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —á–∞—Ç–∞
            
        Returns:
            –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –≤ –≤–∏–¥–µ —Å—Ç—Ä–æ–∫–∏
        """
        try:
            # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
            self.logger.info("üîß –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è Ollama...")
            optimized_messages = self.optimize_text(messages)
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            formatted_text = self.format_messages_for_analysis(optimized_messages)
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è (–ü–û–°–õ–ï –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏)
            if self.llm_logger:
                self.llm_logger.log_formatted_messages(formatted_text, len(optimized_messages))
            
            self.logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è Ollama:")
            self.logger.info(f"   –í—Å–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π: {len(messages)}")
            self.logger.info(f"   –ü–æ—Å–ª–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {len(optimized_messages)}")
            self.logger.info(f"   –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {len(formatted_text)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –í—ã–∑—ã–≤–∞–µ–º Ollama API
            self.logger.info(f"ü§ñ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –≤ Ollama ({self.model})...")
            
            # –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –µ—Å–ª–∏ –ª–æ–≥–≥–µ—Ä —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
            if self.llm_logger:
                self.llm_logger.log_llm_request(formatted_text, "summarization")
            
            import time
            start_time = time.time()
            summary = await self._call_ollama_api(formatted_text)
            end_time = time.time()
            response_time = end_time - start_time
            
            # –õ–æ–≥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –µ—Å–ª–∏ –ª–æ–≥–≥–µ—Ä —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
            if self.llm_logger and summary:
                self.llm_logger.log_llm_response(summary, "summarization", response_time)
                self.llm_logger.log_stage_time('summarization', response_time)
            
            if summary:
                self.logger.info("‚úÖ –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∞ –æ—Ç Ollama")
                return summary
            else:
                self.logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—é–º–µ –æ—Ç Ollama")
                return "‚ùå –û—à–∏–±–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ Ollama"
                
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ Ollama: {e}")
            return f"‚ùå –û—à–∏–±–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏: {str(e)}"
    
    async def is_available(self) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama
        
        Returns:
            True –µ—Å–ª–∏ Ollama –¥–æ—Å—Ç—É–ø–µ–Ω, False –∏–Ω–∞—á–µ
        """
        if not self.validate_config():
            return False
        
        try:
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=10)) as session:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama —Å–µ—Ä–≤–µ—Ä–∞
                async with session.get(f"{self.base_url}/api/tags") as response:
                    if response.status == 200:
                        data = await response.json()
                        models = [model['name'] for model in data.get('models', [])]
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –Ω—É–∂–Ω–∞—è –º–æ–¥–µ–ª—å
                        if self.model in models:
                            self.logger.info(f"‚úÖ Ollama –¥–æ—Å—Ç—É–ø–µ–Ω, –º–æ–¥–µ–ª—å {self.model} –Ω–∞–π–¥–µ–Ω–∞")
                            return True
                        else:
                            self.logger.warning(f"‚ö†Ô∏è Ollama –¥–æ—Å—Ç—É–ø–µ–Ω, –Ω–æ –º–æ–¥–µ–ª—å {self.model} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: {models}")
                            return False
                    else:
                        self.logger.error(f"‚ùå Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, —Å—Ç–∞—Ç—É—Å: {response.status}")
                        return False
                        
        except aiohttp.ClientError as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama: {e}")
            return False
        except Exception as e:
            self.logger.error(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ Ollama: {e}")
            return False
    
    async def generate_response(self, prompt: str) -> Optional[str]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        
        Args:
            prompt: –¢–µ–∫—Å—Ç –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞
            
        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            import time
            start_time = time.time()
            
            self.logger.info(f"ü§ñ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ Ollama –Ω–∞ –ø—Ä–æ–º–ø—Ç –¥–ª–∏–Ω–æ–π {len(prompt)} —Å–∏–º–≤–æ–ª–æ–≤")
            self.logger.debug(f"=== GENERATE_RESPONSE INPUT ===")
            self.logger.debug(f"Prompt length: {len(prompt)}")
            self.logger.debug(f"Prompt preview: {prompt[:200]}...")
            self.logger.debug(f"=== END INPUT ===")
            
            # –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –µ—Å–ª–∏ –ª–æ–≥–≥–µ—Ä —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
            if self.llm_logger:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É –ø—Ä–æ–º–ø—Ç–∞
                request_type = "reflection" if "—Ä–µ—Ñ–ª–µ–∫—Å–∏—è" in prompt.lower() or "–∞–Ω–∞–ª–∏–∑" in prompt.lower() else "improvement"
                self.llm_logger.log_llm_request(prompt, request_type)
            
            response = await self._call_ollama_api(prompt, is_generation=True)
            
            end_time = time.time()
            response_time = end_time - start_time
            
            if response:
                self.logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç Ollama –¥–ª–∏–Ω–æ–π {len(response)} —Å–∏–º–≤–æ–ª–æ–≤ –∑–∞ {response_time:.2f}—Å")
                self.logger.debug(f"=== GENERATE_RESPONSE OUTPUT ===")
                self.logger.debug(f"Response length: {len(response)}")
                self.logger.debug(f"Response preview: {response[:200]}...")
                self.logger.debug(f"=== END OUTPUT ===")
                
                # –õ–æ–≥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –µ—Å–ª–∏ –ª–æ–≥–≥–µ—Ä —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
                if self.llm_logger:
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –æ—Ç–≤–µ—Ç–∞ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É –ø—Ä–æ–º–ø—Ç–∞
                    request_type = "reflection" if "—Ä–µ—Ñ–ª–µ–∫—Å–∏—è" in prompt.lower() or "–∞–Ω–∞–ª–∏–∑" in prompt.lower() else "improvement"
                    self.llm_logger.log_llm_response(response, request_type, response_time)
                    
                    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —ç—Ç–∞–ø–∞
                    if request_type == "reflection":
                        self.llm_logger.log_stage_time('reflection', response_time)
                    elif request_type == "improvement":
                        self.llm_logger.log_stage_time('improvement', response_time)
                
                return response
            else:
                self.logger.warning("‚ö†Ô∏è Ollama –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç")
                return None
                
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ Ollama: {e}")
            return None
    
    def get_provider_info(self) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–µ Ollama
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–µ
        """
        return {
            'name': 'Ollama',
            'display_name': 'Ollama (–õ–æ–∫–∞–ª—å–Ω–∞—è)',
            'description': f'–õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å {self.model} —á–µ—Ä–µ–∑ Ollama',
            'version': self.model,
            'max_tokens': 8000,
            'supports_streaming': True,
            'api_endpoint': self.base_url,
            'provider_type': 'ollama',
            'is_local': True
        }
    
    def validate_config(self) -> bool:
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ Ollama
        
        Returns:
            True –µ—Å–ª–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≤–∞–ª–∏–¥–Ω–∞, False –∏–Ω–∞—á–µ
        """
        if not self.base_url:
            self.logger.error("‚ùå Ollama base URL –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
            return False
        
        if not self.model:
            self.logger.error("‚ùå Ollama –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞")
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ URL –≤—ã–≥–ª—è–¥–∏—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
        if not (self.base_url.startswith('http://') or self.base_url.startswith('https://')):
            self.logger.error("‚ùå Ollama base URL –¥–æ–ª–∂–µ–Ω –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å http:// –∏–ª–∏ https://")
            return False
        
        return True
    
    async def _call_ollama_api(self, text: str, is_generation: bool = False) -> Optional[str]:
        """
        –í—ã–∑–≤–∞—Ç—å Ollama API –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –∏–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            is_generation: True –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, False –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            if is_generation:
                # –î–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –ø—Ä–æ–º–ø—Ç
                prompt = text
            else:
                # –î–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç
                from prompts import PromptTemplates
                prompt = PromptTemplates.get_summarization_prompt(text, 'ollama')

            self.logger.info(f"üîó –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –≤ Ollama")
            self.logger.info(f"üìù –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
            self.logger.info(f"ü§ñ –ú–æ–¥–µ–ª—å: {self.model}")
            
            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "max_tokens": 2000 if is_generation else 1000
                }
            }
            
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=self.timeout)) as session:
                async with session.post(
                    f"{self.base_url}/api/generate",
                    json=payload,
                    headers={"Content-Type": "application/json"}
                ) as response:
                    
                    if response.status == 200:
                        data = await response.json()
                        
                        if 'response' in data:
                            result = data['response'].strip()
                            self.logger.info(f"üì° –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç Ollama: {len(result)} —Å–∏–º–≤–æ–ª–æ–≤")
                            return result
                        else:
                            self.logger.error("‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç Ollama")
                            return None
                    else:
                        error_text = await response.text()
                        self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ Ollama API: {response.status} - {error_text}")
                        return None
                        
        except aiohttp.ClientTimeout:
            self.logger.error(f"‚ùå –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –∫ Ollama (>{self.timeout}—Å)")
            return None
        except aiohttp.ClientError as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama: {e}")
            return None
        except json.JSONDecodeError as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –æ—Ç Ollama: {e}")
            return None
        except Exception as e:
            self.logger.error(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ Ollama: {e}")
            return None
    
    async def get_available_models(self) -> Dict[str, Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π Ollama
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –º–æ–¥–µ–ª—è—Ö
        """
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(f"{self.base_url}/api/tags", timeout=aiohttp.ClientTimeout(total=10)) as response:
                    if response.status == 200:
                        data = await response.json()
                        models = {}
                        
                        if 'models' in data:
                            for model_info in data['models']:
                                model_name = model_info.get('name', '')
                                if model_name:
                                    # –°–æ–∑–¥–∞–µ–º –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º–æ–µ –∏–º—è
                                    display_name = model_name.replace(':', ' ').title()
                                    
                                    models[model_name] = {
                                        'display_name': display_name,
                                        'name': model_name,
                                        'size': model_info.get('size', 0),
                                        'modified_at': model_info.get('modified_at', ''),
                                        'free': True  # –õ–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤—Å–µ–≥–¥–∞ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ
                                    }
                        
                        self.logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(models)} –º–æ–¥–µ–ª–µ–π Ollama")
                        return models
                    else:
                        error_text = await response.text()
                        self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π Ollama: {response.status} - {error_text}")
                        return {}
                        
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π Ollama: {e}")
            return {}
